{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/chengjun/Research/blob/master/GAT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, accuracy\n",
    "from models import GAT, SpGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda=False\n",
    "        self.cuda=True\n",
    "        self.fastmode=False\n",
    "        self.sparse=False\n",
    "        self.seed=72\n",
    "        self.epochs=800\n",
    "        self.lr=5e-3\n",
    "        self.weight_decay=5e-4\n",
    "        self.hidden=8\n",
    "        self.nb_heads=8\n",
    "        self.dropout=0.6\n",
    "        self.alpha=0.2\n",
    "        self.patience=100\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(labels.max()) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9405 acc_train: 0.1786 loss_val: 1.9368 acc_val: 0.3733 time: 1.8959s\n",
      "Epoch: 0002 loss_train: 1.9409 acc_train: 0.2143 loss_val: 1.9270 acc_val: 0.4767 time: 1.8800s\n",
      "Epoch: 0003 loss_train: 1.9264 acc_train: 0.3643 loss_val: 1.9171 acc_val: 0.4933 time: 1.9199s\n",
      "Epoch: 0004 loss_train: 1.8969 acc_train: 0.5000 loss_val: 1.9069 acc_val: 0.5033 time: 1.8750s\n",
      "Epoch: 0005 loss_train: 1.9021 acc_train: 0.4357 loss_val: 1.8965 acc_val: 0.5133 time: 1.9219s\n",
      "Epoch: 0006 loss_train: 1.8796 acc_train: 0.4857 loss_val: 1.8859 acc_val: 0.4900 time: 1.8790s\n",
      "Epoch: 0007 loss_train: 1.8797 acc_train: 0.4714 loss_val: 1.8754 acc_val: 0.5033 time: 1.8969s\n",
      "Epoch: 0008 loss_train: 1.8608 acc_train: 0.5143 loss_val: 1.8648 acc_val: 0.4967 time: 1.9538s\n",
      "Epoch: 0009 loss_train: 1.8491 acc_train: 0.5214 loss_val: 1.8540 acc_val: 0.4933 time: 1.9717s\n",
      "Epoch: 0010 loss_train: 1.8439 acc_train: 0.4786 loss_val: 1.8431 acc_val: 0.4800 time: 1.8949s\n",
      "Epoch: 0011 loss_train: 1.8182 acc_train: 0.4929 loss_val: 1.8321 acc_val: 0.4833 time: 1.9159s\n",
      "Epoch: 0012 loss_train: 1.8069 acc_train: 0.4714 loss_val: 1.8209 acc_val: 0.4800 time: 1.9318s\n",
      "Epoch: 0013 loss_train: 1.7942 acc_train: 0.4500 loss_val: 1.8096 acc_val: 0.4800 time: 2.0016s\n",
      "Epoch: 0014 loss_train: 1.7750 acc_train: 0.4286 loss_val: 1.7982 acc_val: 0.4800 time: 1.8800s\n",
      "Epoch: 0015 loss_train: 1.7638 acc_train: 0.4714 loss_val: 1.7865 acc_val: 0.4800 time: 1.9158s\n",
      "Epoch: 0016 loss_train: 1.7849 acc_train: 0.4357 loss_val: 1.7750 acc_val: 0.4833 time: 1.9378s\n",
      "Epoch: 0017 loss_train: 1.7343 acc_train: 0.4857 loss_val: 1.7635 acc_val: 0.4867 time: 1.9130s\n",
      "Epoch: 0018 loss_train: 1.7395 acc_train: 0.5214 loss_val: 1.7519 acc_val: 0.4900 time: 2.1304s\n",
      "Epoch: 0019 loss_train: 1.7235 acc_train: 0.4929 loss_val: 1.7401 acc_val: 0.4933 time: 1.9189s\n",
      "Epoch: 0020 loss_train: 1.7155 acc_train: 0.4929 loss_val: 1.7284 acc_val: 0.5000 time: 1.9019s\n",
      "Epoch: 0021 loss_train: 1.6920 acc_train: 0.4786 loss_val: 1.7165 acc_val: 0.5033 time: 2.0086s\n",
      "Epoch: 0022 loss_train: 1.7034 acc_train: 0.4714 loss_val: 1.7046 acc_val: 0.5067 time: 1.9957s\n",
      "Epoch: 0023 loss_train: 1.6612 acc_train: 0.5000 loss_val: 1.6926 acc_val: 0.5067 time: 2.1393s\n",
      "Epoch: 0024 loss_train: 1.6677 acc_train: 0.5000 loss_val: 1.6807 acc_val: 0.5067 time: 1.9449s\n",
      "Epoch: 0025 loss_train: 1.6400 acc_train: 0.4714 loss_val: 1.6687 acc_val: 0.5167 time: 1.9089s\n",
      "Epoch: 0026 loss_train: 1.6278 acc_train: 0.4786 loss_val: 1.6569 acc_val: 0.5200 time: 1.8780s\n",
      "Epoch: 0027 loss_train: 1.6028 acc_train: 0.5500 loss_val: 1.6452 acc_val: 0.5267 time: 1.9997s\n",
      "Epoch: 0028 loss_train: 1.5622 acc_train: 0.6000 loss_val: 1.6334 acc_val: 0.5367 time: 1.9757s\n",
      "Epoch: 0029 loss_train: 1.6110 acc_train: 0.5429 loss_val: 1.6217 acc_val: 0.5367 time: 1.9029s\n",
      "Epoch: 0030 loss_train: 1.5366 acc_train: 0.5929 loss_val: 1.6100 acc_val: 0.5533 time: 1.9787s\n",
      "Epoch: 0031 loss_train: 1.5566 acc_train: 0.5500 loss_val: 1.5982 acc_val: 0.5667 time: 1.9767s\n",
      "Epoch: 0032 loss_train: 1.5160 acc_train: 0.5714 loss_val: 1.5866 acc_val: 0.5867 time: 1.9278s\n",
      "Epoch: 0033 loss_train: 1.6098 acc_train: 0.5143 loss_val: 1.5751 acc_val: 0.5967 time: 1.8939s\n",
      "Epoch: 0034 loss_train: 1.5297 acc_train: 0.5571 loss_val: 1.5635 acc_val: 0.6167 time: 1.8999s\n",
      "Epoch: 0035 loss_train: 1.4954 acc_train: 0.5786 loss_val: 1.5521 acc_val: 0.6267 time: 1.9787s\n",
      "Epoch: 0036 loss_train: 1.5029 acc_train: 0.5929 loss_val: 1.5407 acc_val: 0.6300 time: 1.9342s\n",
      "Epoch: 0037 loss_train: 1.4968 acc_train: 0.6357 loss_val: 1.5294 acc_val: 0.6367 time: 1.9468s\n",
      "Epoch: 0038 loss_train: 1.4232 acc_train: 0.6357 loss_val: 1.5182 acc_val: 0.6433 time: 1.9568s\n",
      "Epoch: 0039 loss_train: 1.4612 acc_train: 0.6500 loss_val: 1.5071 acc_val: 0.6600 time: 1.9697s\n",
      "Epoch: 0040 loss_train: 1.4736 acc_train: 0.5857 loss_val: 1.4962 acc_val: 0.6700 time: 1.9159s\n",
      "Epoch: 0041 loss_train: 1.4236 acc_train: 0.5857 loss_val: 1.4854 acc_val: 0.6800 time: 1.9029s\n",
      "Epoch: 0042 loss_train: 1.3795 acc_train: 0.6571 loss_val: 1.4747 acc_val: 0.6867 time: 1.9728s\n",
      "Epoch: 0043 loss_train: 1.3886 acc_train: 0.6500 loss_val: 1.4640 acc_val: 0.6933 time: 1.9911s\n",
      "Epoch: 0044 loss_train: 1.3602 acc_train: 0.6286 loss_val: 1.4537 acc_val: 0.7033 time: 1.9798s\n",
      "Epoch: 0045 loss_train: 1.4366 acc_train: 0.6143 loss_val: 1.4436 acc_val: 0.7033 time: 1.9741s\n",
      "Epoch: 0046 loss_train: 1.3162 acc_train: 0.6714 loss_val: 1.4334 acc_val: 0.7100 time: 1.9159s\n",
      "Epoch: 0047 loss_train: 1.3423 acc_train: 0.6929 loss_val: 1.4236 acc_val: 0.7167 time: 1.8909s\n",
      "Epoch: 0048 loss_train: 1.4253 acc_train: 0.6500 loss_val: 1.4138 acc_val: 0.7200 time: 1.9249s\n",
      "Epoch: 0049 loss_train: 1.3394 acc_train: 0.6429 loss_val: 1.4042 acc_val: 0.7200 time: 1.8939s\n",
      "Epoch: 0050 loss_train: 1.3514 acc_train: 0.7143 loss_val: 1.3948 acc_val: 0.7267 time: 1.9243s\n",
      "Epoch: 0051 loss_train: 1.4084 acc_train: 0.6786 loss_val: 1.3856 acc_val: 0.7433 time: 2.0231s\n",
      "Epoch: 0052 loss_train: 1.3687 acc_train: 0.6786 loss_val: 1.3765 acc_val: 0.7533 time: 1.9663s\n",
      "Epoch: 0053 loss_train: 1.2537 acc_train: 0.7214 loss_val: 1.3677 acc_val: 0.7633 time: 1.8710s\n",
      "Epoch: 0054 loss_train: 1.2264 acc_train: 0.6929 loss_val: 1.3590 acc_val: 0.7667 time: 1.9069s\n",
      "Epoch: 0055 loss_train: 1.2629 acc_train: 0.6857 loss_val: 1.3503 acc_val: 0.7700 time: 1.9219s\n",
      "Epoch: 0056 loss_train: 1.3297 acc_train: 0.6929 loss_val: 1.3416 acc_val: 0.7733 time: 1.9538s\n",
      "Epoch: 0057 loss_train: 1.2068 acc_train: 0.7214 loss_val: 1.3328 acc_val: 0.7767 time: 1.9558s\n",
      "Epoch: 0058 loss_train: 1.2503 acc_train: 0.7000 loss_val: 1.3241 acc_val: 0.7800 time: 1.9418s\n",
      "Epoch: 0059 loss_train: 1.2674 acc_train: 0.7286 loss_val: 1.3154 acc_val: 0.7867 time: 1.9857s\n",
      "Epoch: 0060 loss_train: 1.2394 acc_train: 0.7143 loss_val: 1.3067 acc_val: 0.7900 time: 1.9179s\n",
      "Epoch: 0061 loss_train: 1.2145 acc_train: 0.7214 loss_val: 1.2981 acc_val: 0.7933 time: 1.9219s\n",
      "Epoch: 0062 loss_train: 1.1689 acc_train: 0.7071 loss_val: 1.2896 acc_val: 0.7933 time: 1.9498s\n",
      "Epoch: 0063 loss_train: 1.1090 acc_train: 0.7786 loss_val: 1.2809 acc_val: 0.7967 time: 1.8710s\n",
      "Epoch: 0064 loss_train: 1.2252 acc_train: 0.7071 loss_val: 1.2726 acc_val: 0.8000 time: 1.9598s\n",
      "Epoch: 0065 loss_train: 1.2037 acc_train: 0.6857 loss_val: 1.2643 acc_val: 0.8067 time: 1.9887s\n",
      "Epoch: 0066 loss_train: 1.1560 acc_train: 0.7500 loss_val: 1.2561 acc_val: 0.8000 time: 1.8850s\n",
      "Epoch: 0067 loss_train: 1.2350 acc_train: 0.7357 loss_val: 1.2481 acc_val: 0.8000 time: 1.8810s\n",
      "Epoch: 0068 loss_train: 1.1086 acc_train: 0.7286 loss_val: 1.2400 acc_val: 0.8100 time: 1.9119s\n",
      "Epoch: 0069 loss_train: 1.1782 acc_train: 0.7000 loss_val: 1.2320 acc_val: 0.8133 time: 2.0076s\n",
      "Epoch: 0070 loss_train: 1.2009 acc_train: 0.7143 loss_val: 1.2242 acc_val: 0.8133 time: 1.9258s\n",
      "Epoch: 0071 loss_train: 1.1661 acc_train: 0.7214 loss_val: 1.2163 acc_val: 0.8200 time: 1.8879s\n",
      "Epoch: 0072 loss_train: 1.1862 acc_train: 0.7500 loss_val: 1.2085 acc_val: 0.8200 time: 1.9171s\n",
      "Epoch: 0073 loss_train: 1.1141 acc_train: 0.7429 loss_val: 1.2007 acc_val: 0.8200 time: 1.8959s\n",
      "Epoch: 0074 loss_train: 1.0990 acc_train: 0.7429 loss_val: 1.1930 acc_val: 0.8200 time: 1.9428s\n",
      "Epoch: 0075 loss_train: 1.1849 acc_train: 0.7000 loss_val: 1.1855 acc_val: 0.8200 time: 1.9328s\n",
      "Epoch: 0076 loss_train: 1.1747 acc_train: 0.7000 loss_val: 1.1784 acc_val: 0.8200 time: 1.8929s\n",
      "Epoch: 0077 loss_train: 1.0985 acc_train: 0.7429 loss_val: 1.1713 acc_val: 0.8233 time: 1.9009s\n",
      "Epoch: 0078 loss_train: 1.1574 acc_train: 0.7214 loss_val: 1.1642 acc_val: 0.8233 time: 1.9179s\n",
      "Epoch: 0079 loss_train: 1.1392 acc_train: 0.7500 loss_val: 1.1576 acc_val: 0.8233 time: 1.9189s\n",
      "Epoch: 0080 loss_train: 1.0907 acc_train: 0.7357 loss_val: 1.1510 acc_val: 0.8233 time: 1.9628s\n",
      "Epoch: 0081 loss_train: 1.0660 acc_train: 0.7500 loss_val: 1.1446 acc_val: 0.8233 time: 1.9969s\n",
      "Epoch: 0082 loss_train: 1.1502 acc_train: 0.7071 loss_val: 1.1381 acc_val: 0.8233 time: 1.9069s\n",
      "Epoch: 0083 loss_train: 1.0782 acc_train: 0.7429 loss_val: 1.1316 acc_val: 0.8267 time: 1.8830s\n",
      "Epoch: 0084 loss_train: 1.0695 acc_train: 0.7714 loss_val: 1.1249 acc_val: 0.8267 time: 2.0186s\n",
      "Epoch: 0085 loss_train: 0.9992 acc_train: 0.7643 loss_val: 1.1182 acc_val: 0.8267 time: 1.9298s\n",
      "Epoch: 0086 loss_train: 1.0347 acc_train: 0.7857 loss_val: 1.1115 acc_val: 0.8267 time: 1.9957s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 1.0172 acc_train: 0.7857 loss_val: 1.1048 acc_val: 0.8300 time: 1.9917s\n",
      "Epoch: 0088 loss_train: 1.0394 acc_train: 0.7571 loss_val: 1.0982 acc_val: 0.8300 time: 1.8900s\n",
      "Epoch: 0089 loss_train: 1.0260 acc_train: 0.7643 loss_val: 1.0918 acc_val: 0.8300 time: 1.9907s\n",
      "Epoch: 0090 loss_train: 1.0301 acc_train: 0.7786 loss_val: 1.0855 acc_val: 0.8300 time: 1.9129s\n",
      "Epoch: 0091 loss_train: 1.0136 acc_train: 0.7643 loss_val: 1.0789 acc_val: 0.8300 time: 2.0126s\n",
      "Epoch: 0092 loss_train: 1.0552 acc_train: 0.7643 loss_val: 1.0726 acc_val: 0.8300 time: 1.8999s\n",
      "Epoch: 0093 loss_train: 0.9770 acc_train: 0.8071 loss_val: 1.0663 acc_val: 0.8333 time: 1.9647s\n",
      "Epoch: 0094 loss_train: 0.9803 acc_train: 0.7429 loss_val: 1.0602 acc_val: 0.8333 time: 1.9763s\n",
      "Epoch: 0095 loss_train: 1.0522 acc_train: 0.7714 loss_val: 1.0541 acc_val: 0.8333 time: 1.9169s\n",
      "Epoch: 0096 loss_train: 0.9953 acc_train: 0.7571 loss_val: 1.0481 acc_val: 0.8333 time: 1.9348s\n",
      "Epoch: 0097 loss_train: 1.0536 acc_train: 0.7429 loss_val: 1.0423 acc_val: 0.8333 time: 1.8989s\n",
      "Epoch: 0098 loss_train: 1.0081 acc_train: 0.7571 loss_val: 1.0366 acc_val: 0.8267 time: 1.9209s\n",
      "Epoch: 0099 loss_train: 0.9703 acc_train: 0.7714 loss_val: 1.0311 acc_val: 0.8267 time: 1.9149s\n",
      "Epoch: 0100 loss_train: 0.9358 acc_train: 0.7643 loss_val: 1.0256 acc_val: 0.8300 time: 1.9837s\n",
      "Epoch: 0101 loss_train: 1.0403 acc_train: 0.7643 loss_val: 1.0202 acc_val: 0.8300 time: 1.9408s\n",
      "Epoch: 0102 loss_train: 0.9585 acc_train: 0.7786 loss_val: 1.0148 acc_val: 0.8300 time: 1.9348s\n",
      "Epoch: 0103 loss_train: 0.9703 acc_train: 0.7571 loss_val: 1.0096 acc_val: 0.8333 time: 2.0366s\n",
      "Epoch: 0104 loss_train: 0.9650 acc_train: 0.7429 loss_val: 1.0046 acc_val: 0.8300 time: 1.8580s\n",
      "Epoch: 0105 loss_train: 1.0247 acc_train: 0.7429 loss_val: 0.9995 acc_val: 0.8333 time: 1.9867s\n",
      "Epoch: 0106 loss_train: 0.9509 acc_train: 0.7786 loss_val: 0.9942 acc_val: 0.8333 time: 1.9481s\n",
      "Epoch: 0107 loss_train: 0.9651 acc_train: 0.7786 loss_val: 0.9890 acc_val: 0.8333 time: 1.8840s\n",
      "Epoch: 0108 loss_train: 0.9477 acc_train: 0.7429 loss_val: 0.9839 acc_val: 0.8333 time: 1.9066s\n",
      "Epoch: 0109 loss_train: 1.0373 acc_train: 0.7143 loss_val: 0.9790 acc_val: 0.8367 time: 1.8850s\n",
      "Epoch: 0110 loss_train: 0.9611 acc_train: 0.7786 loss_val: 0.9741 acc_val: 0.8367 time: 2.0435s\n",
      "Epoch: 0111 loss_train: 0.9660 acc_train: 0.7357 loss_val: 0.9694 acc_val: 0.8367 time: 1.9019s\n",
      "Epoch: 0112 loss_train: 0.9240 acc_train: 0.7714 loss_val: 0.9648 acc_val: 0.8367 time: 1.8989s\n",
      "Epoch: 0113 loss_train: 0.8720 acc_train: 0.8071 loss_val: 0.9601 acc_val: 0.8367 time: 1.9458s\n",
      "Epoch: 0114 loss_train: 0.9813 acc_train: 0.7429 loss_val: 0.9555 acc_val: 0.8367 time: 1.9907s\n",
      "Epoch: 0115 loss_train: 0.9325 acc_train: 0.7857 loss_val: 0.9508 acc_val: 0.8367 time: 1.9059s\n",
      "Epoch: 0116 loss_train: 0.8098 acc_train: 0.8214 loss_val: 0.9461 acc_val: 0.8367 time: 2.0211s\n",
      "Epoch: 0117 loss_train: 0.8722 acc_train: 0.8000 loss_val: 0.9415 acc_val: 0.8367 time: 1.9657s\n",
      "Epoch: 0118 loss_train: 0.7808 acc_train: 0.8357 loss_val: 0.9369 acc_val: 0.8367 time: 1.9409s\n",
      "Epoch: 0119 loss_train: 0.9202 acc_train: 0.7214 loss_val: 0.9324 acc_val: 0.8367 time: 2.1004s\n",
      "Epoch: 0120 loss_train: 0.9761 acc_train: 0.7500 loss_val: 0.9281 acc_val: 0.8333 time: 1.9568s\n",
      "Epoch: 0121 loss_train: 0.9233 acc_train: 0.7286 loss_val: 0.9239 acc_val: 0.8367 time: 2.0725s\n",
      "Epoch: 0122 loss_train: 0.8381 acc_train: 0.7571 loss_val: 0.9199 acc_val: 0.8367 time: 1.9777s\n",
      "Epoch: 0123 loss_train: 0.9541 acc_train: 0.7500 loss_val: 0.9159 acc_val: 0.8367 time: 2.0505s\n",
      "Epoch: 0124 loss_train: 0.8594 acc_train: 0.7929 loss_val: 0.9118 acc_val: 0.8367 time: 1.9508s\n",
      "Epoch: 0125 loss_train: 0.8644 acc_train: 0.7643 loss_val: 0.9081 acc_val: 0.8367 time: 1.9807s\n",
      "Epoch: 0126 loss_train: 0.8893 acc_train: 0.8143 loss_val: 0.9044 acc_val: 0.8367 time: 1.9877s\n",
      "Epoch: 0127 loss_train: 0.7666 acc_train: 0.8500 loss_val: 0.9006 acc_val: 0.8367 time: 1.9059s\n",
      "Epoch: 0128 loss_train: 0.8772 acc_train: 0.7857 loss_val: 0.8971 acc_val: 0.8333 time: 1.9268s\n",
      "Epoch: 0129 loss_train: 0.8205 acc_train: 0.7929 loss_val: 0.8937 acc_val: 0.8333 time: 1.8760s\n",
      "Epoch: 0130 loss_train: 0.8782 acc_train: 0.7643 loss_val: 0.8903 acc_val: 0.8333 time: 2.0475s\n",
      "Epoch: 0131 loss_train: 0.7899 acc_train: 0.8071 loss_val: 0.8871 acc_val: 0.8367 time: 1.9348s\n",
      "Epoch: 0132 loss_train: 0.9125 acc_train: 0.7571 loss_val: 0.8839 acc_val: 0.8367 time: 1.8710s\n",
      "Epoch: 0133 loss_train: 0.9380 acc_train: 0.7500 loss_val: 0.8809 acc_val: 0.8367 time: 1.9239s\n",
      "Epoch: 0134 loss_train: 0.8824 acc_train: 0.7857 loss_val: 0.8778 acc_val: 0.8400 time: 1.9927s\n",
      "Epoch: 0135 loss_train: 0.8270 acc_train: 0.7857 loss_val: 0.8749 acc_val: 0.8400 time: 1.9388s\n",
      "Epoch: 0136 loss_train: 0.8456 acc_train: 0.7929 loss_val: 0.8720 acc_val: 0.8400 time: 1.9488s\n",
      "Epoch: 0137 loss_train: 0.7757 acc_train: 0.8286 loss_val: 0.8691 acc_val: 0.8400 time: 1.9593s\n",
      "Epoch: 0138 loss_train: 0.8132 acc_train: 0.7643 loss_val: 0.8661 acc_val: 0.8400 time: 1.9880s\n",
      "Epoch: 0139 loss_train: 0.9230 acc_train: 0.7643 loss_val: 0.8635 acc_val: 0.8400 time: 1.9358s\n",
      "Epoch: 0140 loss_train: 0.8225 acc_train: 0.8143 loss_val: 0.8609 acc_val: 0.8400 time: 1.9907s\n",
      "Epoch: 0141 loss_train: 0.8086 acc_train: 0.7714 loss_val: 0.8585 acc_val: 0.8433 time: 1.8969s\n",
      "Epoch: 0142 loss_train: 0.8896 acc_train: 0.7429 loss_val: 0.8562 acc_val: 0.8400 time: 1.9538s\n",
      "Epoch: 0143 loss_train: 0.9213 acc_train: 0.7500 loss_val: 0.8540 acc_val: 0.8367 time: 1.8830s\n",
      "Epoch: 0144 loss_train: 0.8657 acc_train: 0.8071 loss_val: 0.8518 acc_val: 0.8333 time: 1.9410s\n",
      "Epoch: 0145 loss_train: 0.7895 acc_train: 0.8286 loss_val: 0.8495 acc_val: 0.8333 time: 1.9438s\n",
      "Epoch: 0146 loss_train: 0.8444 acc_train: 0.7643 loss_val: 0.8472 acc_val: 0.8300 time: 1.9308s\n",
      "Epoch: 0147 loss_train: 0.8624 acc_train: 0.7857 loss_val: 0.8451 acc_val: 0.8300 time: 1.9189s\n",
      "Epoch: 0148 loss_train: 0.7828 acc_train: 0.7500 loss_val: 0.8430 acc_val: 0.8333 time: 1.9139s\n",
      "Epoch: 0149 loss_train: 0.8683 acc_train: 0.7214 loss_val: 0.8412 acc_val: 0.8333 time: 1.9026s\n",
      "Epoch: 0150 loss_train: 0.8946 acc_train: 0.7714 loss_val: 0.8396 acc_val: 0.8333 time: 1.9106s\n",
      "Epoch: 0151 loss_train: 0.7975 acc_train: 0.7857 loss_val: 0.8378 acc_val: 0.8333 time: 1.9468s\n",
      "Epoch: 0152 loss_train: 0.7110 acc_train: 0.8000 loss_val: 0.8361 acc_val: 0.8333 time: 1.9468s\n",
      "Epoch: 0153 loss_train: 0.8031 acc_train: 0.8214 loss_val: 0.8343 acc_val: 0.8333 time: 1.9308s\n",
      "Epoch: 0154 loss_train: 0.8454 acc_train: 0.7786 loss_val: 0.8327 acc_val: 0.8367 time: 1.9019s\n",
      "Epoch: 0155 loss_train: 0.7900 acc_train: 0.8071 loss_val: 0.8312 acc_val: 0.8333 time: 1.9578s\n",
      "Epoch: 0156 loss_train: 0.9063 acc_train: 0.7714 loss_val: 0.8299 acc_val: 0.8333 time: 1.9827s\n",
      "Epoch: 0157 loss_train: 0.8447 acc_train: 0.7643 loss_val: 0.8285 acc_val: 0.8333 time: 1.9771s\n",
      "Epoch: 0158 loss_train: 0.7848 acc_train: 0.8071 loss_val: 0.8269 acc_val: 0.8333 time: 1.9428s\n",
      "Epoch: 0159 loss_train: 0.8652 acc_train: 0.8143 loss_val: 0.8253 acc_val: 0.8300 time: 1.9358s\n",
      "Epoch: 0160 loss_train: 0.8362 acc_train: 0.7786 loss_val: 0.8237 acc_val: 0.8300 time: 2.1014s\n",
      "Epoch: 0161 loss_train: 0.7954 acc_train: 0.8071 loss_val: 0.8217 acc_val: 0.8300 time: 1.9807s\n",
      "Epoch: 0162 loss_train: 0.7673 acc_train: 0.7929 loss_val: 0.8196 acc_val: 0.8300 time: 1.9847s\n",
      "Epoch: 0163 loss_train: 0.7882 acc_train: 0.7929 loss_val: 0.8175 acc_val: 0.8300 time: 2.0276s\n",
      "Epoch: 0164 loss_train: 0.8624 acc_train: 0.7429 loss_val: 0.8154 acc_val: 0.8300 time: 1.9308s\n",
      "Epoch: 0165 loss_train: 0.7720 acc_train: 0.7786 loss_val: 0.8131 acc_val: 0.8300 time: 1.9498s\n",
      "Epoch: 0166 loss_train: 0.7480 acc_train: 0.8143 loss_val: 0.8109 acc_val: 0.8300 time: 1.9887s\n",
      "Epoch: 0167 loss_train: 0.7596 acc_train: 0.7857 loss_val: 0.8087 acc_val: 0.8300 time: 2.1163s\n",
      "Epoch: 0168 loss_train: 0.8151 acc_train: 0.7571 loss_val: 0.8067 acc_val: 0.8300 time: 1.9987s\n",
      "Epoch: 0169 loss_train: 0.8218 acc_train: 0.7714 loss_val: 0.8048 acc_val: 0.8300 time: 1.9563s\n",
      "Epoch: 0170 loss_train: 0.8157 acc_train: 0.7929 loss_val: 0.8028 acc_val: 0.8300 time: 1.8939s\n",
      "Epoch: 0171 loss_train: 0.8243 acc_train: 0.7643 loss_val: 0.8008 acc_val: 0.8300 time: 1.9448s\n",
      "Epoch: 0172 loss_train: 0.8263 acc_train: 0.7643 loss_val: 0.7988 acc_val: 0.8267 time: 1.9129s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.6907 acc_train: 0.8286 loss_val: 0.7969 acc_val: 0.8267 time: 1.9757s\n",
      "Epoch: 0174 loss_train: 0.8033 acc_train: 0.8071 loss_val: 0.7950 acc_val: 0.8233 time: 1.9378s\n",
      "Epoch: 0175 loss_train: 0.8068 acc_train: 0.7929 loss_val: 0.7932 acc_val: 0.8233 time: 2.0545s\n",
      "Epoch: 0176 loss_train: 0.7218 acc_train: 0.8429 loss_val: 0.7915 acc_val: 0.8233 time: 2.0292s\n",
      "Epoch: 0177 loss_train: 0.7551 acc_train: 0.8000 loss_val: 0.7898 acc_val: 0.8267 time: 1.9999s\n",
      "Epoch: 0178 loss_train: 0.7345 acc_train: 0.8000 loss_val: 0.7883 acc_val: 0.8267 time: 1.9952s\n",
      "Epoch: 0179 loss_train: 0.6663 acc_train: 0.8714 loss_val: 0.7867 acc_val: 0.8267 time: 1.9592s\n",
      "Epoch: 0180 loss_train: 0.7548 acc_train: 0.7929 loss_val: 0.7851 acc_val: 0.8233 time: 1.9953s\n",
      "Epoch: 0181 loss_train: 0.7679 acc_train: 0.7857 loss_val: 0.7838 acc_val: 0.8233 time: 1.9588s\n",
      "Epoch: 0182 loss_train: 0.7960 acc_train: 0.7857 loss_val: 0.7825 acc_val: 0.8233 time: 1.9221s\n",
      "Epoch: 0183 loss_train: 0.8565 acc_train: 0.7714 loss_val: 0.7811 acc_val: 0.8267 time: 1.9328s\n",
      "Epoch: 0184 loss_train: 0.8402 acc_train: 0.7500 loss_val: 0.7797 acc_val: 0.8267 time: 1.9079s\n",
      "Epoch: 0185 loss_train: 0.7500 acc_train: 0.8357 loss_val: 0.7784 acc_val: 0.8267 time: 1.9079s\n",
      "Epoch: 0186 loss_train: 0.7030 acc_train: 0.8143 loss_val: 0.7772 acc_val: 0.8233 time: 1.9647s\n",
      "Epoch: 0187 loss_train: 0.7290 acc_train: 0.8143 loss_val: 0.7762 acc_val: 0.8233 time: 1.9348s\n",
      "Epoch: 0188 loss_train: 0.8267 acc_train: 0.7500 loss_val: 0.7752 acc_val: 0.8233 time: 1.9169s\n",
      "Epoch: 0189 loss_train: 0.8432 acc_train: 0.7500 loss_val: 0.7743 acc_val: 0.8267 time: 1.9368s\n",
      "Epoch: 0190 loss_train: 0.7113 acc_train: 0.8286 loss_val: 0.7734 acc_val: 0.8267 time: 1.9546s\n",
      "Epoch: 0191 loss_train: 0.8271 acc_train: 0.7857 loss_val: 0.7726 acc_val: 0.8267 time: 1.9647s\n",
      "Epoch: 0192 loss_train: 0.7909 acc_train: 0.8214 loss_val: 0.7716 acc_val: 0.8267 time: 1.9288s\n",
      "Epoch: 0193 loss_train: 0.7477 acc_train: 0.7929 loss_val: 0.7704 acc_val: 0.8267 time: 2.0076s\n",
      "Epoch: 0194 loss_train: 0.7341 acc_train: 0.8143 loss_val: 0.7692 acc_val: 0.8267 time: 2.0934s\n",
      "Epoch: 0195 loss_train: 0.7987 acc_train: 0.7929 loss_val: 0.7685 acc_val: 0.8267 time: 1.9069s\n",
      "Epoch: 0196 loss_train: 0.6502 acc_train: 0.8357 loss_val: 0.7675 acc_val: 0.8267 time: 1.9318s\n",
      "Epoch: 0197 loss_train: 0.7705 acc_train: 0.8214 loss_val: 0.7665 acc_val: 0.8267 time: 1.9937s\n",
      "Epoch: 0198 loss_train: 0.8176 acc_train: 0.7714 loss_val: 0.7653 acc_val: 0.8267 time: 1.9378s\n",
      "Epoch: 0199 loss_train: 0.8121 acc_train: 0.7786 loss_val: 0.7642 acc_val: 0.8267 time: 2.0296s\n",
      "Epoch: 0200 loss_train: 0.7040 acc_train: 0.8214 loss_val: 0.7631 acc_val: 0.8267 time: 1.9867s\n",
      "Epoch: 0201 loss_train: 0.7699 acc_train: 0.8071 loss_val: 0.7619 acc_val: 0.8267 time: 2.0575s\n",
      "Epoch: 0202 loss_train: 0.7312 acc_train: 0.8143 loss_val: 0.7608 acc_val: 0.8233 time: 1.9428s\n",
      "Epoch: 0203 loss_train: 0.6287 acc_train: 0.8500 loss_val: 0.7598 acc_val: 0.8233 time: 2.0123s\n",
      "Epoch: 0204 loss_train: 0.8000 acc_train: 0.7786 loss_val: 0.7587 acc_val: 0.8233 time: 1.9029s\n",
      "Epoch: 0205 loss_train: 0.7556 acc_train: 0.7929 loss_val: 0.7576 acc_val: 0.8233 time: 2.0106s\n",
      "Epoch: 0206 loss_train: 0.7594 acc_train: 0.8214 loss_val: 0.7564 acc_val: 0.8233 time: 1.9159s\n",
      "Epoch: 0207 loss_train: 0.7955 acc_train: 0.7429 loss_val: 0.7554 acc_val: 0.8233 time: 1.9548s\n",
      "Epoch: 0208 loss_train: 0.7909 acc_train: 0.7714 loss_val: 0.7545 acc_val: 0.8233 time: 1.9867s\n",
      "Epoch: 0209 loss_train: 0.8063 acc_train: 0.7571 loss_val: 0.7535 acc_val: 0.8233 time: 1.9787s\n",
      "Epoch: 0210 loss_train: 0.7779 acc_train: 0.7714 loss_val: 0.7526 acc_val: 0.8233 time: 2.0006s\n",
      "Epoch: 0211 loss_train: 0.7173 acc_train: 0.8286 loss_val: 0.7516 acc_val: 0.8267 time: 1.9159s\n",
      "Epoch: 0212 loss_train: 0.6948 acc_train: 0.8071 loss_val: 0.7507 acc_val: 0.8233 time: 1.9059s\n",
      "Epoch: 0213 loss_train: 0.7949 acc_train: 0.7571 loss_val: 0.7500 acc_val: 0.8233 time: 1.8949s\n",
      "Epoch: 0214 loss_train: 0.7500 acc_train: 0.7786 loss_val: 0.7494 acc_val: 0.8233 time: 1.9448s\n",
      "Epoch: 0215 loss_train: 0.6446 acc_train: 0.8571 loss_val: 0.7489 acc_val: 0.8300 time: 1.9418s\n",
      "Epoch: 0216 loss_train: 0.7500 acc_train: 0.7929 loss_val: 0.7483 acc_val: 0.8300 time: 1.9139s\n",
      "Epoch: 0217 loss_train: 0.8110 acc_train: 0.7857 loss_val: 0.7474 acc_val: 0.8267 time: 1.8909s\n",
      "Epoch: 0218 loss_train: 0.6758 acc_train: 0.8429 loss_val: 0.7464 acc_val: 0.8267 time: 1.8720s\n",
      "Epoch: 0219 loss_train: 0.7846 acc_train: 0.7786 loss_val: 0.7453 acc_val: 0.8267 time: 1.9069s\n",
      "Epoch: 0220 loss_train: 0.7165 acc_train: 0.8571 loss_val: 0.7443 acc_val: 0.8267 time: 1.9229s\n",
      "Epoch: 0221 loss_train: 0.6295 acc_train: 0.8786 loss_val: 0.7433 acc_val: 0.8267 time: 1.9817s\n",
      "Epoch: 0222 loss_train: 0.6989 acc_train: 0.8214 loss_val: 0.7423 acc_val: 0.8267 time: 1.9109s\n",
      "Epoch: 0223 loss_train: 0.6942 acc_train: 0.8214 loss_val: 0.7413 acc_val: 0.8233 time: 1.9129s\n",
      "Epoch: 0224 loss_train: 0.6737 acc_train: 0.7929 loss_val: 0.7403 acc_val: 0.8233 time: 1.8979s\n",
      "Epoch: 0225 loss_train: 0.7654 acc_train: 0.7929 loss_val: 0.7394 acc_val: 0.8233 time: 1.9019s\n",
      "Epoch: 0226 loss_train: 0.7810 acc_train: 0.7786 loss_val: 0.7387 acc_val: 0.8233 time: 1.9996s\n",
      "Epoch: 0227 loss_train: 0.7728 acc_train: 0.7929 loss_val: 0.7379 acc_val: 0.8233 time: 1.9598s\n",
      "Epoch: 0228 loss_train: 0.7531 acc_train: 0.8357 loss_val: 0.7371 acc_val: 0.8200 time: 1.9408s\n",
      "Epoch: 0229 loss_train: 0.7627 acc_train: 0.7714 loss_val: 0.7363 acc_val: 0.8200 time: 1.8840s\n",
      "Epoch: 0230 loss_train: 0.7709 acc_train: 0.8000 loss_val: 0.7358 acc_val: 0.8200 time: 1.9129s\n",
      "Epoch: 0231 loss_train: 0.7301 acc_train: 0.8357 loss_val: 0.7352 acc_val: 0.8200 time: 1.8999s\n",
      "Epoch: 0232 loss_train: 0.7224 acc_train: 0.7786 loss_val: 0.7345 acc_val: 0.8167 time: 1.9448s\n",
      "Epoch: 0233 loss_train: 0.6747 acc_train: 0.8429 loss_val: 0.7339 acc_val: 0.8133 time: 1.9089s\n",
      "Epoch: 0234 loss_train: 0.7238 acc_train: 0.7857 loss_val: 0.7335 acc_val: 0.8133 time: 1.9328s\n",
      "Epoch: 0235 loss_train: 0.6806 acc_train: 0.8357 loss_val: 0.7332 acc_val: 0.8200 time: 1.9139s\n",
      "Epoch: 0236 loss_train: 0.7003 acc_train: 0.8143 loss_val: 0.7329 acc_val: 0.8167 time: 1.9388s\n",
      "Epoch: 0237 loss_train: 0.7101 acc_train: 0.8357 loss_val: 0.7325 acc_val: 0.8200 time: 1.9657s\n",
      "Epoch: 0238 loss_train: 0.8111 acc_train: 0.7714 loss_val: 0.7321 acc_val: 0.8200 time: 1.9362s\n",
      "Epoch: 0239 loss_train: 0.8041 acc_train: 0.7857 loss_val: 0.7317 acc_val: 0.8200 time: 1.9318s\n",
      "Epoch: 0240 loss_train: 0.7755 acc_train: 0.8000 loss_val: 0.7316 acc_val: 0.8167 time: 1.9119s\n",
      "Epoch: 0241 loss_train: 0.5740 acc_train: 0.8857 loss_val: 0.7313 acc_val: 0.8167 time: 1.9157s\n",
      "Epoch: 0242 loss_train: 0.6183 acc_train: 0.8143 loss_val: 0.7309 acc_val: 0.8200 time: 1.9767s\n",
      "Epoch: 0243 loss_train: 0.7377 acc_train: 0.7857 loss_val: 0.7306 acc_val: 0.8200 time: 1.9219s\n",
      "Epoch: 0244 loss_train: 0.5724 acc_train: 0.8857 loss_val: 0.7303 acc_val: 0.8200 time: 1.8870s\n",
      "Epoch: 0245 loss_train: 0.6578 acc_train: 0.8143 loss_val: 0.7300 acc_val: 0.8200 time: 1.9049s\n",
      "Epoch: 0246 loss_train: 0.6719 acc_train: 0.8071 loss_val: 0.7296 acc_val: 0.8200 time: 1.9079s\n",
      "Epoch: 0247 loss_train: 0.6138 acc_train: 0.8500 loss_val: 0.7293 acc_val: 0.8200 time: 1.8700s\n",
      "Epoch: 0248 loss_train: 0.7432 acc_train: 0.7643 loss_val: 0.7288 acc_val: 0.8200 time: 1.9478s\n",
      "Epoch: 0249 loss_train: 0.7456 acc_train: 0.7643 loss_val: 0.7282 acc_val: 0.8200 time: 1.9258s\n",
      "Epoch: 0250 loss_train: 0.5771 acc_train: 0.8143 loss_val: 0.7275 acc_val: 0.8200 time: 1.9268s\n",
      "Epoch: 0251 loss_train: 0.6677 acc_train: 0.8071 loss_val: 0.7268 acc_val: 0.8200 time: 1.8540s\n",
      "Epoch: 0252 loss_train: 0.8405 acc_train: 0.7786 loss_val: 0.7262 acc_val: 0.8200 time: 1.9657s\n",
      "Epoch: 0253 loss_train: 0.6226 acc_train: 0.8429 loss_val: 0.7253 acc_val: 0.8200 time: 1.9538s\n",
      "Epoch: 0254 loss_train: 0.7298 acc_train: 0.8286 loss_val: 0.7244 acc_val: 0.8233 time: 1.9229s\n",
      "Epoch: 0255 loss_train: 0.8084 acc_train: 0.8071 loss_val: 0.7233 acc_val: 0.8233 time: 1.8939s\n",
      "Epoch: 0256 loss_train: 0.6808 acc_train: 0.8214 loss_val: 0.7224 acc_val: 0.8233 time: 1.8810s\n",
      "Epoch: 0257 loss_train: 0.7210 acc_train: 0.7714 loss_val: 0.7217 acc_val: 0.8233 time: 1.9139s\n",
      "Epoch: 0258 loss_train: 0.6379 acc_train: 0.8214 loss_val: 0.7208 acc_val: 0.8233 time: 1.9059s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0259 loss_train: 0.6399 acc_train: 0.8143 loss_val: 0.7200 acc_val: 0.8233 time: 1.9777s\n",
      "Epoch: 0260 loss_train: 0.7204 acc_train: 0.8214 loss_val: 0.7191 acc_val: 0.8233 time: 1.9756s\n",
      "Epoch: 0261 loss_train: 0.7189 acc_train: 0.8143 loss_val: 0.7183 acc_val: 0.8233 time: 1.9416s\n",
      "Epoch: 0262 loss_train: 0.7633 acc_train: 0.7643 loss_val: 0.7175 acc_val: 0.8267 time: 1.8882s\n",
      "Epoch: 0263 loss_train: 0.7179 acc_train: 0.7429 loss_val: 0.7169 acc_val: 0.8267 time: 1.9568s\n",
      "Epoch: 0264 loss_train: 0.7484 acc_train: 0.7786 loss_val: 0.7167 acc_val: 0.8267 time: 1.8971s\n",
      "Epoch: 0265 loss_train: 0.7124 acc_train: 0.8143 loss_val: 0.7167 acc_val: 0.8233 time: 1.9578s\n",
      "Epoch: 0266 loss_train: 0.6900 acc_train: 0.7857 loss_val: 0.7168 acc_val: 0.8233 time: 1.9139s\n",
      "Epoch: 0267 loss_train: 0.6235 acc_train: 0.8286 loss_val: 0.7167 acc_val: 0.8233 time: 1.8660s\n",
      "Epoch: 0268 loss_train: 0.7217 acc_train: 0.7786 loss_val: 0.7165 acc_val: 0.8233 time: 1.9189s\n",
      "Epoch: 0269 loss_train: 0.7240 acc_train: 0.8286 loss_val: 0.7162 acc_val: 0.8200 time: 1.8979s\n",
      "Epoch: 0270 loss_train: 0.7195 acc_train: 0.8000 loss_val: 0.7157 acc_val: 0.8200 time: 1.9219s\n",
      "Epoch: 0271 loss_train: 0.6167 acc_train: 0.8429 loss_val: 0.7150 acc_val: 0.8200 time: 1.9444s\n",
      "Epoch: 0272 loss_train: 0.7099 acc_train: 0.8143 loss_val: 0.7149 acc_val: 0.8200 time: 1.9288s\n",
      "Epoch: 0273 loss_train: 0.7553 acc_train: 0.7643 loss_val: 0.7147 acc_val: 0.8167 time: 1.9079s\n",
      "Epoch: 0274 loss_train: 0.6614 acc_train: 0.8429 loss_val: 0.7147 acc_val: 0.8167 time: 1.9209s\n",
      "Epoch: 0275 loss_train: 0.7079 acc_train: 0.7857 loss_val: 0.7150 acc_val: 0.8167 time: 1.9179s\n",
      "Epoch: 0276 loss_train: 0.6871 acc_train: 0.8000 loss_val: 0.7154 acc_val: 0.8133 time: 1.9752s\n",
      "Epoch: 0277 loss_train: 0.7344 acc_train: 0.7786 loss_val: 0.7159 acc_val: 0.8133 time: 1.9996s\n",
      "Epoch: 0278 loss_train: 0.7334 acc_train: 0.7929 loss_val: 0.7163 acc_val: 0.8167 time: 1.9468s\n",
      "Epoch: 0279 loss_train: 0.6732 acc_train: 0.8000 loss_val: 0.7165 acc_val: 0.8167 time: 1.9710s\n",
      "Epoch: 0280 loss_train: 0.6241 acc_train: 0.8429 loss_val: 0.7167 acc_val: 0.8133 time: 2.1025s\n",
      "Epoch: 0281 loss_train: 0.6345 acc_train: 0.8429 loss_val: 0.7166 acc_val: 0.8133 time: 1.9551s\n",
      "Epoch: 0282 loss_train: 0.7292 acc_train: 0.8143 loss_val: 0.7163 acc_val: 0.8100 time: 1.8931s\n",
      "Epoch: 0283 loss_train: 0.7554 acc_train: 0.8071 loss_val: 0.7160 acc_val: 0.8133 time: 1.8600s\n",
      "Epoch: 0284 loss_train: 0.6602 acc_train: 0.8429 loss_val: 0.7154 acc_val: 0.8167 time: 1.9787s\n",
      "Epoch: 0285 loss_train: 0.6919 acc_train: 0.7929 loss_val: 0.7149 acc_val: 0.8167 time: 1.9440s\n",
      "Epoch: 0286 loss_train: 0.6397 acc_train: 0.8357 loss_val: 0.7142 acc_val: 0.8167 time: 1.9229s\n",
      "Epoch: 0287 loss_train: 0.6863 acc_train: 0.8357 loss_val: 0.7138 acc_val: 0.8167 time: 1.9358s\n",
      "Epoch: 0288 loss_train: 0.7564 acc_train: 0.8000 loss_val: 0.7135 acc_val: 0.8167 time: 1.8985s\n",
      "Epoch: 0289 loss_train: 0.7434 acc_train: 0.7786 loss_val: 0.7130 acc_val: 0.8200 time: 1.9169s\n",
      "Epoch: 0290 loss_train: 0.6988 acc_train: 0.7929 loss_val: 0.7127 acc_val: 0.8233 time: 1.9009s\n",
      "Epoch: 0291 loss_train: 0.6468 acc_train: 0.8429 loss_val: 0.7121 acc_val: 0.8233 time: 1.9569s\n",
      "Epoch: 0292 loss_train: 0.7123 acc_train: 0.7786 loss_val: 0.7117 acc_val: 0.8233 time: 1.9170s\n",
      "Epoch: 0293 loss_train: 0.7061 acc_train: 0.8071 loss_val: 0.7114 acc_val: 0.8233 time: 1.9328s\n",
      "Epoch: 0294 loss_train: 0.7530 acc_train: 0.7929 loss_val: 0.7113 acc_val: 0.8200 time: 1.8999s\n",
      "Epoch: 0295 loss_train: 0.7296 acc_train: 0.7786 loss_val: 0.7112 acc_val: 0.8200 time: 1.9428s\n",
      "Epoch: 0296 loss_train: 0.5590 acc_train: 0.8714 loss_val: 0.7107 acc_val: 0.8200 time: 1.8899s\n",
      "Epoch: 0297 loss_train: 0.6768 acc_train: 0.8214 loss_val: 0.7099 acc_val: 0.8200 time: 1.8959s\n",
      "Epoch: 0298 loss_train: 0.7103 acc_train: 0.7857 loss_val: 0.7088 acc_val: 0.8167 time: 1.8847s\n",
      "Epoch: 0299 loss_train: 0.6091 acc_train: 0.8357 loss_val: 0.7079 acc_val: 0.8167 time: 1.9588s\n",
      "Epoch: 0300 loss_train: 0.6698 acc_train: 0.7714 loss_val: 0.7070 acc_val: 0.8167 time: 1.9618s\n",
      "Epoch: 0301 loss_train: 0.7905 acc_train: 0.7786 loss_val: 0.7062 acc_val: 0.8133 time: 1.9209s\n",
      "Epoch: 0302 loss_train: 0.7585 acc_train: 0.7929 loss_val: 0.7057 acc_val: 0.8133 time: 1.8620s\n",
      "Epoch: 0303 loss_train: 0.5966 acc_train: 0.8286 loss_val: 0.7050 acc_val: 0.8133 time: 1.8939s\n",
      "Epoch: 0304 loss_train: 0.6580 acc_train: 0.8500 loss_val: 0.7042 acc_val: 0.8133 time: 1.9049s\n",
      "Epoch: 0305 loss_train: 0.6777 acc_train: 0.7714 loss_val: 0.7036 acc_val: 0.8133 time: 1.9099s\n",
      "Epoch: 0306 loss_train: 0.6521 acc_train: 0.8071 loss_val: 0.7030 acc_val: 0.8133 time: 1.9079s\n",
      "Epoch: 0307 loss_train: 0.6632 acc_train: 0.8143 loss_val: 0.7023 acc_val: 0.8133 time: 1.9628s\n",
      "Epoch: 0308 loss_train: 0.6214 acc_train: 0.8214 loss_val: 0.7017 acc_val: 0.8133 time: 1.9059s\n",
      "Epoch: 0309 loss_train: 0.6531 acc_train: 0.8000 loss_val: 0.7013 acc_val: 0.8167 time: 1.9139s\n",
      "Epoch: 0310 loss_train: 0.6260 acc_train: 0.8500 loss_val: 0.7011 acc_val: 0.8167 time: 1.9458s\n",
      "Epoch: 0311 loss_train: 0.7314 acc_train: 0.8000 loss_val: 0.7007 acc_val: 0.8200 time: 1.9817s\n",
      "Epoch: 0312 loss_train: 0.7672 acc_train: 0.7500 loss_val: 0.7003 acc_val: 0.8233 time: 1.9271s\n",
      "Epoch: 0313 loss_train: 0.5951 acc_train: 0.8500 loss_val: 0.7000 acc_val: 0.8200 time: 1.9801s\n",
      "Epoch: 0314 loss_train: 0.7542 acc_train: 0.7714 loss_val: 0.6998 acc_val: 0.8200 time: 1.9049s\n",
      "Epoch: 0315 loss_train: 0.5891 acc_train: 0.8286 loss_val: 0.6994 acc_val: 0.8200 time: 1.9189s\n",
      "Epoch: 0316 loss_train: 0.6360 acc_train: 0.8071 loss_val: 0.6993 acc_val: 0.8200 time: 1.9139s\n",
      "Epoch: 0317 loss_train: 0.5319 acc_train: 0.8643 loss_val: 0.6990 acc_val: 0.8200 time: 1.9578s\n",
      "Epoch: 0318 loss_train: 0.6687 acc_train: 0.8214 loss_val: 0.6988 acc_val: 0.8233 time: 1.8959s\n",
      "Epoch: 0319 loss_train: 0.5220 acc_train: 0.8429 loss_val: 0.6987 acc_val: 0.8233 time: 1.9219s\n",
      "Epoch: 0320 loss_train: 0.7334 acc_train: 0.7714 loss_val: 0.6987 acc_val: 0.8233 time: 1.9099s\n",
      "Epoch: 0321 loss_train: 0.7320 acc_train: 0.7857 loss_val: 0.6988 acc_val: 0.8267 time: 1.9009s\n",
      "Epoch: 0322 loss_train: 0.7648 acc_train: 0.7429 loss_val: 0.6987 acc_val: 0.8267 time: 1.9199s\n",
      "Epoch: 0323 loss_train: 0.6387 acc_train: 0.8000 loss_val: 0.6986 acc_val: 0.8267 time: 1.9029s\n",
      "Epoch: 0324 loss_train: 0.5768 acc_train: 0.8571 loss_val: 0.6983 acc_val: 0.8267 time: 1.9328s\n",
      "Epoch: 0325 loss_train: 0.7567 acc_train: 0.8000 loss_val: 0.6980 acc_val: 0.8267 time: 1.9019s\n",
      "Epoch: 0326 loss_train: 0.7299 acc_train: 0.8143 loss_val: 0.6977 acc_val: 0.8233 time: 1.9199s\n",
      "Epoch: 0327 loss_train: 0.6796 acc_train: 0.8071 loss_val: 0.6976 acc_val: 0.8233 time: 1.9249s\n",
      "Epoch: 0328 loss_train: 0.7509 acc_train: 0.7643 loss_val: 0.6974 acc_val: 0.8233 time: 1.9458s\n",
      "Epoch: 0329 loss_train: 0.6547 acc_train: 0.8214 loss_val: 0.6975 acc_val: 0.8233 time: 2.0256s\n",
      "Epoch: 0330 loss_train: 0.6813 acc_train: 0.8214 loss_val: 0.6977 acc_val: 0.8233 time: 1.9807s\n",
      "Epoch: 0331 loss_train: 0.6725 acc_train: 0.8286 loss_val: 0.6977 acc_val: 0.8233 time: 2.0006s\n",
      "Epoch: 0332 loss_train: 0.6633 acc_train: 0.8214 loss_val: 0.6979 acc_val: 0.8233 time: 1.9328s\n",
      "Epoch: 0333 loss_train: 0.7128 acc_train: 0.8214 loss_val: 0.6980 acc_val: 0.8233 time: 1.9558s\n",
      "Epoch: 0334 loss_train: 0.7405 acc_train: 0.7571 loss_val: 0.6983 acc_val: 0.8233 time: 1.9268s\n",
      "Epoch: 0335 loss_train: 0.6626 acc_train: 0.8143 loss_val: 0.6983 acc_val: 0.8233 time: 1.9418s\n",
      "Epoch: 0336 loss_train: 0.6825 acc_train: 0.8214 loss_val: 0.6981 acc_val: 0.8233 time: 1.9339s\n",
      "Epoch: 0337 loss_train: 0.6239 acc_train: 0.8357 loss_val: 0.6980 acc_val: 0.8233 time: 1.9368s\n",
      "Epoch: 0338 loss_train: 0.7671 acc_train: 0.7714 loss_val: 0.6976 acc_val: 0.8233 time: 1.9019s\n",
      "Epoch: 0339 loss_train: 0.6386 acc_train: 0.8214 loss_val: 0.6972 acc_val: 0.8233 time: 1.9677s\n",
      "Epoch: 0340 loss_train: 0.6968 acc_train: 0.7714 loss_val: 0.6970 acc_val: 0.8267 time: 2.0016s\n",
      "Epoch: 0341 loss_train: 0.6187 acc_train: 0.8214 loss_val: 0.6970 acc_val: 0.8267 time: 1.9229s\n",
      "Epoch: 0342 loss_train: 0.6411 acc_train: 0.8286 loss_val: 0.6968 acc_val: 0.8267 time: 1.9049s\n",
      "Epoch: 0343 loss_train: 0.6139 acc_train: 0.8286 loss_val: 0.6967 acc_val: 0.8267 time: 1.8909s\n",
      "Epoch: 0344 loss_train: 0.6116 acc_train: 0.8214 loss_val: 0.6965 acc_val: 0.8300 time: 1.9308s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0345 loss_train: 0.7463 acc_train: 0.7286 loss_val: 0.6964 acc_val: 0.8300 time: 1.9149s\n",
      "Epoch: 0346 loss_train: 0.7097 acc_train: 0.8000 loss_val: 0.6965 acc_val: 0.8300 time: 1.9466s\n",
      "Epoch: 0347 loss_train: 0.6897 acc_train: 0.7857 loss_val: 0.6966 acc_val: 0.8300 time: 1.9874s\n",
      "Epoch: 0348 loss_train: 0.7091 acc_train: 0.7643 loss_val: 0.6963 acc_val: 0.8300 time: 1.9708s\n",
      "Epoch: 0349 loss_train: 0.5971 acc_train: 0.8571 loss_val: 0.6960 acc_val: 0.8300 time: 1.9695s\n",
      "Epoch: 0350 loss_train: 0.6285 acc_train: 0.8286 loss_val: 0.6958 acc_val: 0.8300 time: 2.0216s\n",
      "Epoch: 0351 loss_train: 0.7100 acc_train: 0.7929 loss_val: 0.6955 acc_val: 0.8300 time: 1.9249s\n",
      "Epoch: 0352 loss_train: 0.7112 acc_train: 0.7857 loss_val: 0.6954 acc_val: 0.8300 time: 1.9719s\n",
      "Epoch: 0353 loss_train: 0.7061 acc_train: 0.8071 loss_val: 0.6951 acc_val: 0.8300 time: 1.9742s\n",
      "Epoch: 0354 loss_train: 0.6072 acc_train: 0.8214 loss_val: 0.6949 acc_val: 0.8300 time: 1.8939s\n",
      "Epoch: 0355 loss_train: 0.6743 acc_train: 0.7714 loss_val: 0.6949 acc_val: 0.8300 time: 1.8879s\n",
      "Epoch: 0356 loss_train: 0.6657 acc_train: 0.8214 loss_val: 0.6949 acc_val: 0.8300 time: 1.9159s\n",
      "Epoch: 0357 loss_train: 0.6291 acc_train: 0.8286 loss_val: 0.6948 acc_val: 0.8300 time: 1.8670s\n",
      "Epoch: 0358 loss_train: 0.4947 acc_train: 0.8571 loss_val: 0.6947 acc_val: 0.8233 time: 1.9069s\n",
      "Epoch: 0359 loss_train: 0.7243 acc_train: 0.8071 loss_val: 0.6948 acc_val: 0.8233 time: 1.9119s\n",
      "Epoch: 0360 loss_train: 0.6650 acc_train: 0.7857 loss_val: 0.6949 acc_val: 0.8233 time: 1.9059s\n",
      "Epoch: 0361 loss_train: 0.5883 acc_train: 0.8500 loss_val: 0.6948 acc_val: 0.8233 time: 1.8720s\n",
      "Epoch: 0362 loss_train: 0.6426 acc_train: 0.7929 loss_val: 0.6949 acc_val: 0.8300 time: 1.8775s\n",
      "Epoch: 0363 loss_train: 0.6758 acc_train: 0.7857 loss_val: 0.6949 acc_val: 0.8300 time: 1.9757s\n",
      "Epoch: 0364 loss_train: 0.6373 acc_train: 0.8214 loss_val: 0.6951 acc_val: 0.8333 time: 1.9079s\n",
      "Epoch: 0365 loss_train: 0.6872 acc_train: 0.8071 loss_val: 0.6949 acc_val: 0.8300 time: 1.8860s\n",
      "Epoch: 0366 loss_train: 0.7431 acc_train: 0.8071 loss_val: 0.6948 acc_val: 0.8233 time: 1.9024s\n",
      "Epoch: 0367 loss_train: 0.6272 acc_train: 0.8429 loss_val: 0.6946 acc_val: 0.8267 time: 1.9019s\n",
      "Epoch: 0368 loss_train: 0.6144 acc_train: 0.8000 loss_val: 0.6943 acc_val: 0.8267 time: 1.9249s\n",
      "Epoch: 0369 loss_train: 0.6348 acc_train: 0.8643 loss_val: 0.6942 acc_val: 0.8267 time: 1.9548s\n",
      "Epoch: 0370 loss_train: 0.6877 acc_train: 0.8357 loss_val: 0.6937 acc_val: 0.8267 time: 1.9169s\n",
      "Epoch: 0371 loss_train: 0.7644 acc_train: 0.7643 loss_val: 0.6932 acc_val: 0.8267 time: 2.0236s\n",
      "Epoch: 0372 loss_train: 0.5966 acc_train: 0.8214 loss_val: 0.6926 acc_val: 0.8267 time: 1.9468s\n",
      "Epoch: 0373 loss_train: 0.7879 acc_train: 0.7143 loss_val: 0.6923 acc_val: 0.8267 time: 1.9528s\n",
      "Epoch: 0374 loss_train: 0.7323 acc_train: 0.7714 loss_val: 0.6919 acc_val: 0.8267 time: 1.8810s\n",
      "Epoch: 0375 loss_train: 0.7005 acc_train: 0.7786 loss_val: 0.6915 acc_val: 0.8300 time: 1.9637s\n",
      "Epoch: 0376 loss_train: 0.5274 acc_train: 0.9071 loss_val: 0.6913 acc_val: 0.8300 time: 1.9258s\n",
      "Epoch: 0377 loss_train: 0.6921 acc_train: 0.8143 loss_val: 0.6910 acc_val: 0.8300 time: 1.9089s\n",
      "Epoch: 0378 loss_train: 0.7162 acc_train: 0.7643 loss_val: 0.6905 acc_val: 0.8300 time: 1.9817s\n",
      "Epoch: 0379 loss_train: 0.6822 acc_train: 0.7786 loss_val: 0.6902 acc_val: 0.8300 time: 1.9508s\n",
      "Epoch: 0380 loss_train: 0.6434 acc_train: 0.8143 loss_val: 0.6899 acc_val: 0.8267 time: 1.8850s\n",
      "Epoch: 0381 loss_train: 0.6639 acc_train: 0.8143 loss_val: 0.6894 acc_val: 0.8233 time: 1.9328s\n",
      "Epoch: 0382 loss_train: 0.6618 acc_train: 0.8357 loss_val: 0.6888 acc_val: 0.8233 time: 1.9169s\n",
      "Epoch: 0383 loss_train: 0.7387 acc_train: 0.7357 loss_val: 0.6884 acc_val: 0.8267 time: 2.0016s\n",
      "Epoch: 0384 loss_train: 0.6821 acc_train: 0.8214 loss_val: 0.6878 acc_val: 0.8267 time: 1.8820s\n",
      "Epoch: 0385 loss_train: 0.6036 acc_train: 0.8214 loss_val: 0.6872 acc_val: 0.8267 time: 1.9578s\n",
      "Epoch: 0386 loss_train: 0.6394 acc_train: 0.8000 loss_val: 0.6870 acc_val: 0.8267 time: 1.8750s\n",
      "Epoch: 0387 loss_train: 0.6208 acc_train: 0.8143 loss_val: 0.6869 acc_val: 0.8233 time: 1.9618s\n",
      "Epoch: 0388 loss_train: 0.5696 acc_train: 0.8500 loss_val: 0.6866 acc_val: 0.8200 time: 1.8999s\n",
      "Epoch: 0389 loss_train: 0.7266 acc_train: 0.7500 loss_val: 0.6865 acc_val: 0.8200 time: 1.8889s\n",
      "Epoch: 0390 loss_train: 0.6794 acc_train: 0.8286 loss_val: 0.6863 acc_val: 0.8200 time: 1.9627s\n",
      "Epoch: 0391 loss_train: 0.6009 acc_train: 0.8500 loss_val: 0.6861 acc_val: 0.8200 time: 1.9358s\n",
      "Epoch: 0392 loss_train: 0.5957 acc_train: 0.8571 loss_val: 0.6861 acc_val: 0.8200 time: 2.0218s\n",
      "Epoch: 0393 loss_train: 0.5717 acc_train: 0.8571 loss_val: 0.6859 acc_val: 0.8200 time: 1.9730s\n",
      "Epoch: 0394 loss_train: 0.6806 acc_train: 0.8214 loss_val: 0.6858 acc_val: 0.8167 time: 1.9121s\n",
      "Epoch: 0395 loss_train: 0.5758 acc_train: 0.8714 loss_val: 0.6858 acc_val: 0.8133 time: 1.9298s\n",
      "Epoch: 0396 loss_train: 0.7334 acc_train: 0.7357 loss_val: 0.6857 acc_val: 0.8133 time: 1.9368s\n",
      "Epoch: 0397 loss_train: 0.6790 acc_train: 0.8286 loss_val: 0.6857 acc_val: 0.8133 time: 1.9258s\n",
      "Epoch: 0398 loss_train: 0.6313 acc_train: 0.8357 loss_val: 0.6858 acc_val: 0.8133 time: 1.9559s\n",
      "Epoch: 0399 loss_train: 0.6136 acc_train: 0.8429 loss_val: 0.6858 acc_val: 0.8133 time: 1.8828s\n",
      "Epoch: 0400 loss_train: 0.5884 acc_train: 0.8500 loss_val: 0.6859 acc_val: 0.8133 time: 1.9209s\n",
      "Epoch: 0401 loss_train: 0.6234 acc_train: 0.8143 loss_val: 0.6860 acc_val: 0.8167 time: 1.8889s\n",
      "Epoch: 0402 loss_train: 0.5810 acc_train: 0.8214 loss_val: 0.6860 acc_val: 0.8167 time: 1.9338s\n",
      "Epoch: 0403 loss_train: 0.5908 acc_train: 0.8571 loss_val: 0.6862 acc_val: 0.8167 time: 1.9308s\n",
      "Epoch: 0404 loss_train: 0.5835 acc_train: 0.8429 loss_val: 0.6863 acc_val: 0.8167 time: 1.9209s\n",
      "Epoch: 0405 loss_train: 0.6623 acc_train: 0.8286 loss_val: 0.6865 acc_val: 0.8100 time: 1.9229s\n",
      "Epoch: 0406 loss_train: 0.6825 acc_train: 0.8071 loss_val: 0.6869 acc_val: 0.8100 time: 1.9468s\n",
      "Epoch: 0407 loss_train: 0.6003 acc_train: 0.8357 loss_val: 0.6872 acc_val: 0.8133 time: 1.9006s\n",
      "Epoch: 0408 loss_train: 0.6482 acc_train: 0.8357 loss_val: 0.6872 acc_val: 0.8200 time: 1.9608s\n",
      "Epoch: 0409 loss_train: 0.5783 acc_train: 0.8714 loss_val: 0.6871 acc_val: 0.8233 time: 1.8658s\n",
      "Epoch: 0410 loss_train: 0.6846 acc_train: 0.7929 loss_val: 0.6869 acc_val: 0.8233 time: 1.9548s\n",
      "Epoch: 0411 loss_train: 0.6259 acc_train: 0.7857 loss_val: 0.6868 acc_val: 0.8233 time: 1.9458s\n",
      "Epoch: 0412 loss_train: 0.6506 acc_train: 0.8214 loss_val: 0.6863 acc_val: 0.8200 time: 1.9408s\n",
      "Epoch: 0413 loss_train: 0.5751 acc_train: 0.8571 loss_val: 0.6857 acc_val: 0.8200 time: 1.9498s\n",
      "Epoch: 0414 loss_train: 0.6716 acc_train: 0.8071 loss_val: 0.6852 acc_val: 0.8200 time: 1.9538s\n",
      "Epoch: 0415 loss_train: 0.5872 acc_train: 0.8286 loss_val: 0.6843 acc_val: 0.8200 time: 1.8952s\n",
      "Epoch: 0416 loss_train: 0.7678 acc_train: 0.7429 loss_val: 0.6836 acc_val: 0.8200 time: 1.9049s\n",
      "Epoch: 0417 loss_train: 0.6728 acc_train: 0.8000 loss_val: 0.6825 acc_val: 0.8200 time: 1.9129s\n",
      "Epoch: 0418 loss_train: 0.5417 acc_train: 0.8214 loss_val: 0.6816 acc_val: 0.8200 time: 1.9747s\n",
      "Epoch: 0419 loss_train: 0.7482 acc_train: 0.7643 loss_val: 0.6811 acc_val: 0.8200 time: 1.9066s\n",
      "Epoch: 0420 loss_train: 0.6576 acc_train: 0.7929 loss_val: 0.6805 acc_val: 0.8200 time: 2.0104s\n",
      "Epoch: 0421 loss_train: 0.5953 acc_train: 0.8500 loss_val: 0.6797 acc_val: 0.8200 time: 1.9320s\n",
      "Epoch: 0422 loss_train: 0.5599 acc_train: 0.8429 loss_val: 0.6789 acc_val: 0.8200 time: 1.9498s\n",
      "Epoch: 0423 loss_train: 0.6623 acc_train: 0.8429 loss_val: 0.6779 acc_val: 0.8200 time: 1.9488s\n",
      "Epoch: 0424 loss_train: 0.7368 acc_train: 0.7571 loss_val: 0.6770 acc_val: 0.8200 time: 1.9637s\n",
      "Epoch: 0425 loss_train: 0.6354 acc_train: 0.7929 loss_val: 0.6759 acc_val: 0.8233 time: 1.9635s\n",
      "Epoch: 0426 loss_train: 0.5934 acc_train: 0.8143 loss_val: 0.6749 acc_val: 0.8200 time: 1.9528s\n",
      "Epoch: 0427 loss_train: 0.6073 acc_train: 0.8071 loss_val: 0.6738 acc_val: 0.8233 time: 1.9209s\n",
      "Epoch: 0428 loss_train: 0.6110 acc_train: 0.8143 loss_val: 0.6730 acc_val: 0.8233 time: 1.9528s\n",
      "Epoch: 0429 loss_train: 0.6114 acc_train: 0.8143 loss_val: 0.6724 acc_val: 0.8233 time: 1.9498s\n",
      "Epoch: 0430 loss_train: 0.6526 acc_train: 0.8143 loss_val: 0.6721 acc_val: 0.8233 time: 1.9458s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0431 loss_train: 0.6642 acc_train: 0.8071 loss_val: 0.6717 acc_val: 0.8233 time: 1.9628s\n",
      "Epoch: 0432 loss_train: 0.6207 acc_train: 0.8357 loss_val: 0.6715 acc_val: 0.8233 time: 1.9298s\n",
      "Epoch: 0433 loss_train: 0.6235 acc_train: 0.8357 loss_val: 0.6712 acc_val: 0.8233 time: 1.8570s\n",
      "Epoch: 0434 loss_train: 0.6272 acc_train: 0.8357 loss_val: 0.6709 acc_val: 0.8233 time: 1.9378s\n",
      "Epoch: 0435 loss_train: 0.7320 acc_train: 0.7786 loss_val: 0.6706 acc_val: 0.8233 time: 1.9887s\n",
      "Epoch: 0436 loss_train: 0.6260 acc_train: 0.8214 loss_val: 0.6705 acc_val: 0.8233 time: 1.9667s\n",
      "Epoch: 0437 loss_train: 0.5773 acc_train: 0.8214 loss_val: 0.6703 acc_val: 0.8233 time: 1.9378s\n",
      "Epoch: 0438 loss_train: 0.6275 acc_train: 0.8286 loss_val: 0.6701 acc_val: 0.8200 time: 1.9817s\n",
      "Epoch: 0439 loss_train: 0.6445 acc_train: 0.8143 loss_val: 0.6700 acc_val: 0.8200 time: 1.9318s\n",
      "Epoch: 0440 loss_train: 0.6419 acc_train: 0.8071 loss_val: 0.6697 acc_val: 0.8200 time: 1.9268s\n",
      "Epoch: 0441 loss_train: 0.6745 acc_train: 0.7857 loss_val: 0.6693 acc_val: 0.8200 time: 1.9239s\n",
      "Epoch: 0442 loss_train: 0.6814 acc_train: 0.8000 loss_val: 0.6689 acc_val: 0.8167 time: 1.8840s\n",
      "Epoch: 0443 loss_train: 0.6337 acc_train: 0.8000 loss_val: 0.6687 acc_val: 0.8167 time: 1.9268s\n",
      "Epoch: 0444 loss_train: 0.6461 acc_train: 0.8286 loss_val: 0.6686 acc_val: 0.8167 time: 1.9408s\n",
      "Epoch: 0445 loss_train: 0.6873 acc_train: 0.7500 loss_val: 0.6686 acc_val: 0.8133 time: 1.9707s\n",
      "Epoch: 0446 loss_train: 0.5373 acc_train: 0.8571 loss_val: 0.6687 acc_val: 0.8167 time: 1.9009s\n",
      "Epoch: 0447 loss_train: 0.6168 acc_train: 0.8643 loss_val: 0.6685 acc_val: 0.8133 time: 1.9777s\n",
      "Epoch: 0448 loss_train: 0.7595 acc_train: 0.7857 loss_val: 0.6689 acc_val: 0.8100 time: 1.9138s\n",
      "Epoch: 0449 loss_train: 0.6496 acc_train: 0.7857 loss_val: 0.6693 acc_val: 0.8100 time: 1.9049s\n",
      "Epoch: 0450 loss_train: 0.5790 acc_train: 0.8286 loss_val: 0.6694 acc_val: 0.8100 time: 1.9558s\n",
      "Epoch: 0451 loss_train: 0.6948 acc_train: 0.8000 loss_val: 0.6702 acc_val: 0.8100 time: 1.9019s\n",
      "Epoch: 0452 loss_train: 0.5922 acc_train: 0.8286 loss_val: 0.6710 acc_val: 0.8100 time: 2.0006s\n",
      "Epoch: 0453 loss_train: 0.6449 acc_train: 0.8143 loss_val: 0.6717 acc_val: 0.8133 time: 1.8949s\n",
      "Epoch: 0454 loss_train: 0.5578 acc_train: 0.8571 loss_val: 0.6721 acc_val: 0.8067 time: 1.9598s\n",
      "Epoch: 0455 loss_train: 0.6622 acc_train: 0.8571 loss_val: 0.6723 acc_val: 0.8067 time: 1.9348s\n",
      "Epoch: 0456 loss_train: 0.4812 acc_train: 0.8857 loss_val: 0.6722 acc_val: 0.8067 time: 1.9627s\n",
      "Epoch: 0457 loss_train: 0.5544 acc_train: 0.8286 loss_val: 0.6721 acc_val: 0.8100 time: 1.9079s\n",
      "Epoch: 0458 loss_train: 0.6065 acc_train: 0.8357 loss_val: 0.6722 acc_val: 0.8100 time: 1.9179s\n",
      "Epoch: 0459 loss_train: 0.5488 acc_train: 0.8500 loss_val: 0.6720 acc_val: 0.8100 time: 1.9837s\n",
      "Epoch: 0460 loss_train: 0.5425 acc_train: 0.8929 loss_val: 0.6713 acc_val: 0.8133 time: 1.9498s\n",
      "Epoch: 0461 loss_train: 0.6480 acc_train: 0.8214 loss_val: 0.6703 acc_val: 0.8100 time: 1.9149s\n",
      "Epoch: 0462 loss_train: 0.5985 acc_train: 0.8429 loss_val: 0.6694 acc_val: 0.8100 time: 1.8939s\n",
      "Epoch: 0463 loss_train: 0.6483 acc_train: 0.8286 loss_val: 0.6685 acc_val: 0.8067 time: 1.9179s\n",
      "Epoch: 0464 loss_train: 0.7826 acc_train: 0.7500 loss_val: 0.6680 acc_val: 0.8067 time: 2.0166s\n",
      "Epoch: 0465 loss_train: 0.6371 acc_train: 0.7929 loss_val: 0.6674 acc_val: 0.8067 time: 1.9917s\n",
      "Epoch: 0466 loss_train: 0.7144 acc_train: 0.7929 loss_val: 0.6670 acc_val: 0.8067 time: 1.9548s\n",
      "Epoch: 0467 loss_train: 0.5809 acc_train: 0.8714 loss_val: 0.6665 acc_val: 0.8067 time: 2.0026s\n",
      "Epoch: 0468 loss_train: 0.5901 acc_train: 0.8143 loss_val: 0.6665 acc_val: 0.8067 time: 1.9298s\n",
      "Epoch: 0469 loss_train: 0.6702 acc_train: 0.7929 loss_val: 0.6667 acc_val: 0.8067 time: 1.9288s\n",
      "Epoch: 0470 loss_train: 0.5827 acc_train: 0.8357 loss_val: 0.6671 acc_val: 0.8067 time: 1.9548s\n",
      "Epoch: 0471 loss_train: 0.6542 acc_train: 0.8214 loss_val: 0.6673 acc_val: 0.8067 time: 1.9239s\n",
      "Epoch: 0472 loss_train: 0.7206 acc_train: 0.7929 loss_val: 0.6673 acc_val: 0.8067 time: 1.8899s\n",
      "Epoch: 0473 loss_train: 0.7438 acc_train: 0.7571 loss_val: 0.6677 acc_val: 0.8067 time: 1.9079s\n",
      "Epoch: 0474 loss_train: 0.5437 acc_train: 0.8429 loss_val: 0.6681 acc_val: 0.8067 time: 1.9009s\n",
      "Epoch: 0475 loss_train: 0.5287 acc_train: 0.8857 loss_val: 0.6684 acc_val: 0.8067 time: 1.8899s\n",
      "Epoch: 0476 loss_train: 0.6767 acc_train: 0.7786 loss_val: 0.6688 acc_val: 0.8100 time: 1.9468s\n",
      "Epoch: 0477 loss_train: 0.6435 acc_train: 0.8571 loss_val: 0.6689 acc_val: 0.8100 time: 1.9408s\n",
      "Epoch: 0478 loss_train: 0.6837 acc_train: 0.7571 loss_val: 0.6694 acc_val: 0.8100 time: 1.9019s\n",
      "Epoch: 0479 loss_train: 0.6918 acc_train: 0.8071 loss_val: 0.6695 acc_val: 0.8100 time: 1.8899s\n",
      "Epoch: 0480 loss_train: 0.6134 acc_train: 0.8500 loss_val: 0.6695 acc_val: 0.8100 time: 1.9448s\n",
      "Epoch: 0481 loss_train: 0.6547 acc_train: 0.8000 loss_val: 0.6696 acc_val: 0.8100 time: 1.9348s\n",
      "Epoch: 0482 loss_train: 0.7022 acc_train: 0.7786 loss_val: 0.6698 acc_val: 0.8100 time: 1.9039s\n",
      "Epoch: 0483 loss_train: 0.7779 acc_train: 0.7429 loss_val: 0.6697 acc_val: 0.8133 time: 1.9139s\n",
      "Epoch: 0484 loss_train: 0.6832 acc_train: 0.8143 loss_val: 0.6698 acc_val: 0.8133 time: 2.0326s\n",
      "Epoch: 0485 loss_train: 0.7073 acc_train: 0.7643 loss_val: 0.6698 acc_val: 0.8133 time: 1.9594s\n",
      "Epoch: 0486 loss_train: 0.6361 acc_train: 0.8429 loss_val: 0.6698 acc_val: 0.8100 time: 1.9871s\n",
      "Epoch: 0487 loss_train: 0.6088 acc_train: 0.8786 loss_val: 0.6695 acc_val: 0.8133 time: 1.9727s\n",
      "Epoch: 0488 loss_train: 0.6043 acc_train: 0.8357 loss_val: 0.6691 acc_val: 0.8133 time: 1.9431s\n",
      "Epoch: 0489 loss_train: 0.6668 acc_train: 0.8286 loss_val: 0.6689 acc_val: 0.8133 time: 1.9278s\n",
      "Epoch: 0490 loss_train: 0.5686 acc_train: 0.8571 loss_val: 0.6688 acc_val: 0.8167 time: 1.9169s\n",
      "Epoch: 0491 loss_train: 0.5444 acc_train: 0.8429 loss_val: 0.6687 acc_val: 0.8233 time: 1.9408s\n",
      "Epoch: 0492 loss_train: 0.6097 acc_train: 0.8500 loss_val: 0.6688 acc_val: 0.8233 time: 1.9219s\n",
      "Epoch: 0493 loss_train: 0.6759 acc_train: 0.7714 loss_val: 0.6687 acc_val: 0.8233 time: 1.9737s\n",
      "Epoch: 0494 loss_train: 0.6420 acc_train: 0.8286 loss_val: 0.6684 acc_val: 0.8233 time: 1.9588s\n",
      "Epoch: 0495 loss_train: 0.4873 acc_train: 0.8571 loss_val: 0.6681 acc_val: 0.8200 time: 1.9720s\n",
      "Epoch: 0496 loss_train: 0.5805 acc_train: 0.8286 loss_val: 0.6676 acc_val: 0.8233 time: 1.9657s\n",
      "Epoch: 0497 loss_train: 0.6250 acc_train: 0.8286 loss_val: 0.6673 acc_val: 0.8200 time: 1.9039s\n",
      "Epoch: 0498 loss_train: 0.6264 acc_train: 0.7857 loss_val: 0.6671 acc_val: 0.8200 time: 1.9618s\n",
      "Epoch: 0499 loss_train: 0.6429 acc_train: 0.7929 loss_val: 0.6669 acc_val: 0.8200 time: 1.9687s\n",
      "Epoch: 0500 loss_train: 0.6109 acc_train: 0.8429 loss_val: 0.6667 acc_val: 0.8200 time: 1.8961s\n",
      "Epoch: 0501 loss_train: 0.5923 acc_train: 0.8286 loss_val: 0.6667 acc_val: 0.8200 time: 1.9717s\n",
      "Epoch: 0502 loss_train: 0.4993 acc_train: 0.8786 loss_val: 0.6669 acc_val: 0.8200 time: 2.0356s\n",
      "Epoch: 0503 loss_train: 0.6660 acc_train: 0.8071 loss_val: 0.6666 acc_val: 0.8200 time: 2.0056s\n",
      "Epoch: 0504 loss_train: 0.6200 acc_train: 0.8000 loss_val: 0.6663 acc_val: 0.8200 time: 1.9900s\n",
      "Epoch: 0505 loss_train: 0.5345 acc_train: 0.8643 loss_val: 0.6659 acc_val: 0.8200 time: 2.0625s\n",
      "Epoch: 0506 loss_train: 0.6268 acc_train: 0.7857 loss_val: 0.6655 acc_val: 0.8200 time: 1.9847s\n",
      "Epoch: 0507 loss_train: 0.5963 acc_train: 0.8286 loss_val: 0.6651 acc_val: 0.8200 time: 1.9847s\n",
      "Epoch: 0508 loss_train: 0.5666 acc_train: 0.8571 loss_val: 0.6648 acc_val: 0.8200 time: 1.9154s\n",
      "Epoch: 0509 loss_train: 0.6436 acc_train: 0.8357 loss_val: 0.6643 acc_val: 0.8200 time: 1.9654s\n",
      "Epoch: 0510 loss_train: 0.5262 acc_train: 0.8357 loss_val: 0.6640 acc_val: 0.8200 time: 2.0128s\n",
      "Epoch: 0511 loss_train: 0.5836 acc_train: 0.8500 loss_val: 0.6637 acc_val: 0.8200 time: 2.0641s\n",
      "Epoch: 0512 loss_train: 0.6605 acc_train: 0.7714 loss_val: 0.6630 acc_val: 0.8133 time: 2.1014s\n",
      "Epoch: 0513 loss_train: 0.6088 acc_train: 0.8571 loss_val: 0.6628 acc_val: 0.8133 time: 2.0034s\n",
      "Epoch: 0514 loss_train: 0.6747 acc_train: 0.8357 loss_val: 0.6631 acc_val: 0.8133 time: 1.9538s\n",
      "Epoch: 0515 loss_train: 0.6247 acc_train: 0.8286 loss_val: 0.6634 acc_val: 0.8100 time: 1.9328s\n",
      "Epoch: 0516 loss_train: 0.7017 acc_train: 0.7786 loss_val: 0.6635 acc_val: 0.8100 time: 1.9576s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0517 loss_train: 0.6256 acc_train: 0.8214 loss_val: 0.6640 acc_val: 0.8100 time: 1.9047s\n",
      "Epoch: 0518 loss_train: 0.5524 acc_train: 0.8500 loss_val: 0.6643 acc_val: 0.8133 time: 1.9784s\n",
      "Epoch: 0519 loss_train: 0.6083 acc_train: 0.8429 loss_val: 0.6649 acc_val: 0.8133 time: 2.0537s\n",
      "Epoch: 0520 loss_train: 0.6728 acc_train: 0.8000 loss_val: 0.6655 acc_val: 0.8133 time: 1.9664s\n",
      "Epoch: 0521 loss_train: 0.5656 acc_train: 0.8643 loss_val: 0.6662 acc_val: 0.8133 time: 2.0276s\n",
      "Epoch: 0522 loss_train: 0.6135 acc_train: 0.8000 loss_val: 0.6669 acc_val: 0.8133 time: 2.0426s\n",
      "Epoch: 0523 loss_train: 0.6115 acc_train: 0.8500 loss_val: 0.6676 acc_val: 0.8100 time: 2.0510s\n",
      "Epoch: 0524 loss_train: 0.6125 acc_train: 0.8143 loss_val: 0.6680 acc_val: 0.8100 time: 2.0071s\n",
      "Epoch: 0525 loss_train: 0.6122 acc_train: 0.8357 loss_val: 0.6685 acc_val: 0.8133 time: 1.9825s\n",
      "Epoch: 0526 loss_train: 0.5611 acc_train: 0.8500 loss_val: 0.6689 acc_val: 0.8167 time: 2.0987s\n",
      "Epoch: 0527 loss_train: 0.6145 acc_train: 0.8429 loss_val: 0.6693 acc_val: 0.8167 time: 1.9857s\n",
      "Epoch: 0528 loss_train: 0.6135 acc_train: 0.8286 loss_val: 0.6698 acc_val: 0.8167 time: 2.0076s\n",
      "Epoch: 0529 loss_train: 0.6368 acc_train: 0.8357 loss_val: 0.6705 acc_val: 0.8133 time: 2.0196s\n",
      "Epoch: 0530 loss_train: 0.6669 acc_train: 0.8143 loss_val: 0.6714 acc_val: 0.8133 time: 2.0076s\n",
      "Epoch: 0531 loss_train: 0.5304 acc_train: 0.8714 loss_val: 0.6721 acc_val: 0.8133 time: 2.0504s\n",
      "Epoch: 0532 loss_train: 0.6548 acc_train: 0.8357 loss_val: 0.6727 acc_val: 0.8133 time: 2.0056s\n",
      "Epoch: 0533 loss_train: 0.6555 acc_train: 0.8286 loss_val: 0.6727 acc_val: 0.8133 time: 2.0724s\n",
      "Epoch: 0534 loss_train: 0.7146 acc_train: 0.7571 loss_val: 0.6725 acc_val: 0.8133 time: 2.0465s\n",
      "Epoch: 0535 loss_train: 0.6544 acc_train: 0.8214 loss_val: 0.6721 acc_val: 0.8133 time: 1.9785s\n",
      "Epoch: 0536 loss_train: 0.6302 acc_train: 0.8357 loss_val: 0.6718 acc_val: 0.8167 time: 1.9572s\n",
      "Epoch: 0537 loss_train: 0.5890 acc_train: 0.8500 loss_val: 0.6714 acc_val: 0.8167 time: 1.9868s\n",
      "Epoch: 0538 loss_train: 0.7383 acc_train: 0.7786 loss_val: 0.6709 acc_val: 0.8167 time: 1.9102s\n",
      "Epoch: 0539 loss_train: 0.5626 acc_train: 0.8357 loss_val: 0.6702 acc_val: 0.8200 time: 1.9258s\n",
      "Epoch: 0540 loss_train: 0.6342 acc_train: 0.8214 loss_val: 0.6699 acc_val: 0.8200 time: 1.9532s\n",
      "Epoch: 0541 loss_train: 0.5659 acc_train: 0.8500 loss_val: 0.6695 acc_val: 0.8200 time: 1.8770s\n",
      "Epoch: 0542 loss_train: 0.5924 acc_train: 0.8429 loss_val: 0.6688 acc_val: 0.8200 time: 1.9224s\n",
      "Epoch: 0543 loss_train: 0.6571 acc_train: 0.8071 loss_val: 0.6680 acc_val: 0.8200 time: 1.9578s\n",
      "Epoch: 0544 loss_train: 0.6177 acc_train: 0.8357 loss_val: 0.6672 acc_val: 0.8200 time: 1.9777s\n",
      "Epoch: 0545 loss_train: 0.6543 acc_train: 0.8214 loss_val: 0.6664 acc_val: 0.8200 time: 2.0327s\n",
      "Epoch: 0546 loss_train: 0.6222 acc_train: 0.8214 loss_val: 0.6656 acc_val: 0.8167 time: 1.9219s\n",
      "Epoch: 0547 loss_train: 0.6259 acc_train: 0.7786 loss_val: 0.6651 acc_val: 0.8167 time: 1.9159s\n",
      "Epoch: 0548 loss_train: 0.6937 acc_train: 0.7714 loss_val: 0.6648 acc_val: 0.8167 time: 1.9747s\n",
      "Epoch: 0549 loss_train: 0.5591 acc_train: 0.8357 loss_val: 0.6646 acc_val: 0.8167 time: 2.0395s\n",
      "Epoch: 0550 loss_train: 0.6547 acc_train: 0.8071 loss_val: 0.6643 acc_val: 0.8167 time: 1.9978s\n",
      "Epoch: 0551 loss_train: 0.7256 acc_train: 0.7857 loss_val: 0.6639 acc_val: 0.8167 time: 1.9458s\n",
      "Epoch: 0552 loss_train: 0.6489 acc_train: 0.8500 loss_val: 0.6634 acc_val: 0.8167 time: 2.0036s\n",
      "Epoch: 0553 loss_train: 0.6891 acc_train: 0.7857 loss_val: 0.6625 acc_val: 0.8233 time: 1.9408s\n",
      "Epoch: 0554 loss_train: 0.6514 acc_train: 0.8286 loss_val: 0.6617 acc_val: 0.8300 time: 2.0257s\n",
      "Epoch: 0555 loss_train: 0.5734 acc_train: 0.8643 loss_val: 0.6609 acc_val: 0.8267 time: 1.9348s\n",
      "Epoch: 0556 loss_train: 0.5927 acc_train: 0.8286 loss_val: 0.6605 acc_val: 0.8267 time: 1.9378s\n",
      "Epoch: 0557 loss_train: 0.6164 acc_train: 0.8143 loss_val: 0.6602 acc_val: 0.8267 time: 1.9987s\n",
      "Epoch: 0558 loss_train: 0.6248 acc_train: 0.8214 loss_val: 0.6599 acc_val: 0.8267 time: 2.0076s\n",
      "Epoch: 0559 loss_train: 0.6709 acc_train: 0.8000 loss_val: 0.6597 acc_val: 0.8267 time: 2.0196s\n",
      "Epoch: 0560 loss_train: 0.6353 acc_train: 0.8429 loss_val: 0.6597 acc_val: 0.8267 time: 2.0078s\n",
      "Epoch: 0561 loss_train: 0.5504 acc_train: 0.8643 loss_val: 0.6597 acc_val: 0.8267 time: 1.9511s\n",
      "Epoch: 0562 loss_train: 0.5847 acc_train: 0.8643 loss_val: 0.6599 acc_val: 0.8267 time: 1.8797s\n",
      "Epoch: 0563 loss_train: 0.5913 acc_train: 0.8214 loss_val: 0.6599 acc_val: 0.8267 time: 1.9787s\n",
      "Epoch: 0564 loss_train: 0.6637 acc_train: 0.8357 loss_val: 0.6601 acc_val: 0.8267 time: 1.9914s\n",
      "Epoch: 0565 loss_train: 0.5832 acc_train: 0.8000 loss_val: 0.6604 acc_val: 0.8267 time: 1.9398s\n",
      "Epoch: 0566 loss_train: 0.5857 acc_train: 0.8214 loss_val: 0.6609 acc_val: 0.8267 time: 1.8760s\n",
      "Epoch: 0567 loss_train: 0.5894 acc_train: 0.8500 loss_val: 0.6615 acc_val: 0.8267 time: 1.9608s\n",
      "Epoch: 0568 loss_train: 0.7077 acc_train: 0.7929 loss_val: 0.6623 acc_val: 0.8233 time: 1.8955s\n",
      "Epoch: 0569 loss_train: 0.6764 acc_train: 0.7929 loss_val: 0.6631 acc_val: 0.8267 time: 1.9229s\n",
      "Epoch: 0570 loss_train: 0.6944 acc_train: 0.7929 loss_val: 0.6641 acc_val: 0.8233 time: 1.9787s\n",
      "Epoch: 0571 loss_train: 0.5842 acc_train: 0.8214 loss_val: 0.6653 acc_val: 0.8233 time: 1.9268s\n",
      "Epoch: 0572 loss_train: 0.6877 acc_train: 0.7929 loss_val: 0.6663 acc_val: 0.8233 time: 2.0041s\n",
      "Epoch: 0573 loss_train: 0.5824 acc_train: 0.8429 loss_val: 0.6671 acc_val: 0.8233 time: 1.9805s\n",
      "Epoch: 0574 loss_train: 0.5869 acc_train: 0.8214 loss_val: 0.6676 acc_val: 0.8233 time: 1.9468s\n",
      "Epoch: 0575 loss_train: 0.5809 acc_train: 0.8500 loss_val: 0.6679 acc_val: 0.8233 time: 1.9478s\n",
      "Epoch: 0576 loss_train: 0.7074 acc_train: 0.8214 loss_val: 0.6683 acc_val: 0.8233 time: 1.9221s\n",
      "Epoch: 0577 loss_train: 0.5625 acc_train: 0.8643 loss_val: 0.6687 acc_val: 0.8233 time: 1.9199s\n",
      "Epoch: 0578 loss_train: 0.5972 acc_train: 0.8214 loss_val: 0.6691 acc_val: 0.8200 time: 1.9867s\n",
      "Epoch: 0579 loss_train: 0.5760 acc_train: 0.8429 loss_val: 0.6691 acc_val: 0.8200 time: 2.0356s\n",
      "Epoch: 0580 loss_train: 0.5583 acc_train: 0.8643 loss_val: 0.6688 acc_val: 0.8200 time: 1.9953s\n",
      "Epoch: 0581 loss_train: 0.6036 acc_train: 0.8357 loss_val: 0.6681 acc_val: 0.8200 time: 1.9657s\n",
      "Epoch: 0582 loss_train: 0.6158 acc_train: 0.8357 loss_val: 0.6672 acc_val: 0.8200 time: 1.9428s\n",
      "Epoch: 0583 loss_train: 0.6306 acc_train: 0.8214 loss_val: 0.6663 acc_val: 0.8200 time: 1.9388s\n",
      "Epoch: 0584 loss_train: 0.5511 acc_train: 0.8643 loss_val: 0.6654 acc_val: 0.8233 time: 1.9139s\n",
      "Epoch: 0585 loss_train: 0.6369 acc_train: 0.8143 loss_val: 0.6644 acc_val: 0.8233 time: 1.9891s\n",
      "Epoch: 0586 loss_train: 0.6839 acc_train: 0.8143 loss_val: 0.6633 acc_val: 0.8233 time: 1.9105s\n",
      "Epoch: 0587 loss_train: 0.6240 acc_train: 0.8071 loss_val: 0.6627 acc_val: 0.8233 time: 1.9987s\n",
      "Epoch: 0588 loss_train: 0.6365 acc_train: 0.8357 loss_val: 0.6620 acc_val: 0.8233 time: 2.0260s\n",
      "Epoch: 0589 loss_train: 0.5884 acc_train: 0.7857 loss_val: 0.6612 acc_val: 0.8200 time: 2.0078s\n",
      "Epoch: 0590 loss_train: 0.6016 acc_train: 0.8286 loss_val: 0.6607 acc_val: 0.8200 time: 2.0465s\n",
      "Epoch: 0591 loss_train: 0.5145 acc_train: 0.8571 loss_val: 0.6603 acc_val: 0.8200 time: 1.9930s\n",
      "Epoch: 0592 loss_train: 0.5903 acc_train: 0.8500 loss_val: 0.6597 acc_val: 0.8200 time: 1.9598s\n",
      "Epoch: 0593 loss_train: 0.5858 acc_train: 0.8429 loss_val: 0.6592 acc_val: 0.8267 time: 1.9568s\n",
      "Epoch: 0594 loss_train: 0.5824 acc_train: 0.8143 loss_val: 0.6587 acc_val: 0.8267 time: 2.1739s\n",
      "Epoch: 0595 loss_train: 0.6330 acc_train: 0.7929 loss_val: 0.6582 acc_val: 0.8267 time: 1.9667s\n",
      "Epoch: 0596 loss_train: 0.5759 acc_train: 0.8357 loss_val: 0.6579 acc_val: 0.8267 time: 1.9434s\n",
      "Epoch: 0597 loss_train: 0.6956 acc_train: 0.7857 loss_val: 0.6575 acc_val: 0.8267 time: 1.9199s\n",
      "Epoch: 0598 loss_train: 0.5835 acc_train: 0.8500 loss_val: 0.6570 acc_val: 0.8267 time: 1.9558s\n",
      "Epoch: 0599 loss_train: 0.6733 acc_train: 0.7714 loss_val: 0.6565 acc_val: 0.8267 time: 1.9508s\n",
      "Epoch: 0600 loss_train: 0.6624 acc_train: 0.7500 loss_val: 0.6565 acc_val: 0.8267 time: 1.9291s\n",
      "Epoch: 0601 loss_train: 0.5812 acc_train: 0.8571 loss_val: 0.6567 acc_val: 0.8267 time: 1.9069s\n",
      "Epoch: 0602 loss_train: 0.5522 acc_train: 0.8500 loss_val: 0.6571 acc_val: 0.8267 time: 1.9558s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0603 loss_train: 0.6924 acc_train: 0.8286 loss_val: 0.6575 acc_val: 0.8267 time: 1.9249s\n",
      "Epoch: 0604 loss_train: 0.6165 acc_train: 0.8286 loss_val: 0.6578 acc_val: 0.8267 time: 1.9980s\n",
      "Epoch: 0605 loss_train: 0.6045 acc_train: 0.8500 loss_val: 0.6579 acc_val: 0.8267 time: 1.9548s\n",
      "Epoch: 0606 loss_train: 0.5579 acc_train: 0.8571 loss_val: 0.6579 acc_val: 0.8233 time: 2.0017s\n",
      "Epoch: 0607 loss_train: 0.6240 acc_train: 0.8071 loss_val: 0.6577 acc_val: 0.8233 time: 1.9428s\n",
      "Epoch: 0608 loss_train: 0.6166 acc_train: 0.8286 loss_val: 0.6576 acc_val: 0.8233 time: 1.9109s\n",
      "Epoch: 0609 loss_train: 0.6192 acc_train: 0.8143 loss_val: 0.6573 acc_val: 0.8233 time: 1.9972s\n",
      "Epoch: 0610 loss_train: 0.6795 acc_train: 0.7857 loss_val: 0.6572 acc_val: 0.8233 time: 2.0078s\n",
      "Epoch: 0611 loss_train: 0.6018 acc_train: 0.8286 loss_val: 0.6570 acc_val: 0.8233 time: 1.9498s\n",
      "Epoch: 0612 loss_train: 0.5646 acc_train: 0.8429 loss_val: 0.6572 acc_val: 0.8233 time: 1.9754s\n",
      "Epoch: 0613 loss_train: 0.6323 acc_train: 0.8000 loss_val: 0.6571 acc_val: 0.8233 time: 1.9139s\n",
      "Epoch: 0614 loss_train: 0.5647 acc_train: 0.8571 loss_val: 0.6568 acc_val: 0.8233 time: 1.9182s\n",
      "Epoch: 0615 loss_train: 0.6222 acc_train: 0.8071 loss_val: 0.6568 acc_val: 0.8233 time: 1.9051s\n",
      "Epoch: 0616 loss_train: 0.6724 acc_train: 0.8000 loss_val: 0.6567 acc_val: 0.8267 time: 1.9588s\n",
      "Epoch: 0617 loss_train: 0.5680 acc_train: 0.8286 loss_val: 0.6569 acc_val: 0.8267 time: 1.9608s\n",
      "Epoch: 0618 loss_train: 0.6472 acc_train: 0.8000 loss_val: 0.6571 acc_val: 0.8233 time: 1.9358s\n",
      "Epoch: 0619 loss_train: 0.6697 acc_train: 0.7786 loss_val: 0.6572 acc_val: 0.8233 time: 1.9124s\n",
      "Epoch: 0620 loss_train: 0.5664 acc_train: 0.8357 loss_val: 0.6577 acc_val: 0.8233 time: 1.9388s\n",
      "Epoch: 0621 loss_train: 0.5466 acc_train: 0.8714 loss_val: 0.6580 acc_val: 0.8200 time: 1.9169s\n",
      "Epoch: 0622 loss_train: 0.6621 acc_train: 0.7643 loss_val: 0.6582 acc_val: 0.8200 time: 1.9089s\n",
      "Epoch: 0623 loss_train: 0.5964 acc_train: 0.8286 loss_val: 0.6584 acc_val: 0.8200 time: 1.9418s\n",
      "Epoch: 0624 loss_train: 0.5817 acc_train: 0.8500 loss_val: 0.6583 acc_val: 0.8200 time: 1.9090s\n",
      "Epoch: 0625 loss_train: 0.5670 acc_train: 0.8500 loss_val: 0.6580 acc_val: 0.8233 time: 1.9917s\n",
      "Epoch: 0626 loss_train: 0.5880 acc_train: 0.8214 loss_val: 0.6580 acc_val: 0.8233 time: 1.9917s\n",
      "Epoch: 0627 loss_train: 0.6137 acc_train: 0.8000 loss_val: 0.6578 acc_val: 0.8233 time: 2.0076s\n",
      "Epoch: 0628 loss_train: 0.6495 acc_train: 0.7929 loss_val: 0.6577 acc_val: 0.8233 time: 1.9398s\n",
      "Epoch: 0629 loss_train: 0.5932 acc_train: 0.8143 loss_val: 0.6578 acc_val: 0.8233 time: 1.8969s\n",
      "Epoch: 0630 loss_train: 0.5317 acc_train: 0.8571 loss_val: 0.6580 acc_val: 0.8267 time: 1.8850s\n",
      "Epoch: 0631 loss_train: 0.6735 acc_train: 0.8071 loss_val: 0.6583 acc_val: 0.8267 time: 1.8919s\n",
      "Epoch: 0632 loss_train: 0.5330 acc_train: 0.8786 loss_val: 0.6585 acc_val: 0.8233 time: 1.8929s\n",
      "Epoch: 0633 loss_train: 0.5757 acc_train: 0.8429 loss_val: 0.6588 acc_val: 0.8233 time: 1.9159s\n",
      "Epoch: 0634 loss_train: 0.5172 acc_train: 0.8357 loss_val: 0.6591 acc_val: 0.8233 time: 1.9538s\n",
      "Epoch: 0635 loss_train: 0.6291 acc_train: 0.8286 loss_val: 0.6595 acc_val: 0.8233 time: 2.0116s\n",
      "Epoch: 0636 loss_train: 0.6707 acc_train: 0.7786 loss_val: 0.6597 acc_val: 0.8233 time: 1.9328s\n",
      "Epoch: 0637 loss_train: 0.6596 acc_train: 0.8000 loss_val: 0.6600 acc_val: 0.8233 time: 1.9308s\n",
      "Epoch: 0638 loss_train: 0.6211 acc_train: 0.8429 loss_val: 0.6602 acc_val: 0.8233 time: 1.8977s\n",
      "Epoch: 0639 loss_train: 0.5590 acc_train: 0.8500 loss_val: 0.6600 acc_val: 0.8233 time: 1.8850s\n",
      "Epoch: 0640 loss_train: 0.6224 acc_train: 0.8071 loss_val: 0.6601 acc_val: 0.8233 time: 2.0026s\n",
      "Epoch: 0641 loss_train: 0.7626 acc_train: 0.7500 loss_val: 0.6607 acc_val: 0.8233 time: 1.9289s\n",
      "Epoch: 0642 loss_train: 0.6927 acc_train: 0.7714 loss_val: 0.6613 acc_val: 0.8233 time: 2.0289s\n",
      "Epoch: 0643 loss_train: 0.6713 acc_train: 0.8143 loss_val: 0.6618 acc_val: 0.8233 time: 1.9893s\n",
      "Epoch: 0644 loss_train: 0.8003 acc_train: 0.7571 loss_val: 0.6626 acc_val: 0.8200 time: 2.0270s\n",
      "Epoch: 0645 loss_train: 0.6625 acc_train: 0.8071 loss_val: 0.6635 acc_val: 0.8200 time: 1.9771s\n",
      "Epoch: 0646 loss_train: 0.5317 acc_train: 0.8571 loss_val: 0.6641 acc_val: 0.8200 time: 2.0610s\n",
      "Epoch: 0647 loss_train: 0.6075 acc_train: 0.8000 loss_val: 0.6646 acc_val: 0.8167 time: 1.9386s\n",
      "Epoch: 0648 loss_train: 0.5733 acc_train: 0.8429 loss_val: 0.6651 acc_val: 0.8167 time: 1.9089s\n",
      "Epoch: 0649 loss_train: 0.6728 acc_train: 0.7929 loss_val: 0.6656 acc_val: 0.8167 time: 1.9657s\n",
      "Epoch: 0650 loss_train: 0.5479 acc_train: 0.8643 loss_val: 0.6659 acc_val: 0.8167 time: 2.0236s\n",
      "Epoch: 0651 loss_train: 0.6099 acc_train: 0.7857 loss_val: 0.6659 acc_val: 0.8167 time: 1.9428s\n",
      "Epoch: 0652 loss_train: 0.5318 acc_train: 0.8429 loss_val: 0.6664 acc_val: 0.8200 time: 2.0116s\n",
      "Epoch: 0653 loss_train: 0.6256 acc_train: 0.8143 loss_val: 0.6668 acc_val: 0.8200 time: 1.9608s\n",
      "Epoch: 0654 loss_train: 0.5235 acc_train: 0.8429 loss_val: 0.6667 acc_val: 0.8233 time: 1.9069s\n",
      "Epoch: 0655 loss_train: 0.6232 acc_train: 0.8143 loss_val: 0.6666 acc_val: 0.8233 time: 1.9778s\n",
      "Epoch: 0656 loss_train: 0.6297 acc_train: 0.8286 loss_val: 0.6663 acc_val: 0.8233 time: 1.9249s\n",
      "Epoch: 0657 loss_train: 0.6462 acc_train: 0.7929 loss_val: 0.6658 acc_val: 0.8200 time: 1.9400s\n",
      "Epoch: 0658 loss_train: 0.5652 acc_train: 0.8286 loss_val: 0.6651 acc_val: 0.8200 time: 1.9897s\n",
      "Epoch: 0659 loss_train: 0.5929 acc_train: 0.8143 loss_val: 0.6643 acc_val: 0.8200 time: 1.9398s\n",
      "Epoch: 0660 loss_train: 0.7261 acc_train: 0.7929 loss_val: 0.6638 acc_val: 0.8200 time: 1.9737s\n",
      "Epoch: 0661 loss_train: 0.5181 acc_train: 0.8643 loss_val: 0.6630 acc_val: 0.8233 time: 1.9288s\n",
      "Epoch: 0662 loss_train: 0.6402 acc_train: 0.8071 loss_val: 0.6623 acc_val: 0.8233 time: 1.9558s\n",
      "Epoch: 0663 loss_train: 0.5624 acc_train: 0.8500 loss_val: 0.6614 acc_val: 0.8233 time: 2.0136s\n",
      "Epoch: 0664 loss_train: 0.5783 acc_train: 0.8429 loss_val: 0.6607 acc_val: 0.8233 time: 1.9528s\n",
      "Epoch: 0665 loss_train: 0.6932 acc_train: 0.7929 loss_val: 0.6603 acc_val: 0.8233 time: 1.9907s\n",
      "Epoch: 0666 loss_train: 0.5569 acc_train: 0.8429 loss_val: 0.6600 acc_val: 0.8233 time: 1.9498s\n",
      "Epoch: 0667 loss_train: 0.6148 acc_train: 0.8214 loss_val: 0.6596 acc_val: 0.8233 time: 1.8929s\n",
      "Epoch: 0668 loss_train: 0.6467 acc_train: 0.7714 loss_val: 0.6592 acc_val: 0.8233 time: 1.9728s\n",
      "Epoch: 0669 loss_train: 0.5902 acc_train: 0.8286 loss_val: 0.6590 acc_val: 0.8233 time: 1.9445s\n",
      "Epoch: 0670 loss_train: 0.6800 acc_train: 0.7857 loss_val: 0.6586 acc_val: 0.8233 time: 1.9966s\n",
      "Epoch: 0671 loss_train: 0.6524 acc_train: 0.7786 loss_val: 0.6583 acc_val: 0.8233 time: 1.9218s\n",
      "Epoch: 0672 loss_train: 0.6149 acc_train: 0.8143 loss_val: 0.6581 acc_val: 0.8233 time: 1.9071s\n",
      "Epoch: 0673 loss_train: 0.6956 acc_train: 0.7643 loss_val: 0.6585 acc_val: 0.8233 time: 1.9494s\n",
      "Epoch: 0674 loss_train: 0.5282 acc_train: 0.8286 loss_val: 0.6583 acc_val: 0.8267 time: 1.9498s\n",
      "Epoch: 0675 loss_train: 0.6022 acc_train: 0.8143 loss_val: 0.6580 acc_val: 0.8267 time: 2.0004s\n",
      "Epoch: 0676 loss_train: 0.6044 acc_train: 0.8071 loss_val: 0.6574 acc_val: 0.8300 time: 2.0018s\n",
      "Epoch: 0677 loss_train: 0.5668 acc_train: 0.8571 loss_val: 0.6567 acc_val: 0.8300 time: 1.9488s\n",
      "Epoch: 0678 loss_train: 0.5856 acc_train: 0.8214 loss_val: 0.6562 acc_val: 0.8300 time: 1.9687s\n",
      "Epoch: 0679 loss_train: 0.6144 acc_train: 0.8071 loss_val: 0.6558 acc_val: 0.8300 time: 2.0316s\n",
      "Epoch: 0680 loss_train: 0.6333 acc_train: 0.7929 loss_val: 0.6558 acc_val: 0.8300 time: 1.9049s\n",
      "Epoch: 0681 loss_train: 0.6248 acc_train: 0.8429 loss_val: 0.6558 acc_val: 0.8300 time: 1.9234s\n",
      "Epoch: 0682 loss_train: 0.6988 acc_train: 0.7571 loss_val: 0.6557 acc_val: 0.8300 time: 1.9697s\n",
      "Epoch: 0683 loss_train: 0.6219 acc_train: 0.8214 loss_val: 0.6559 acc_val: 0.8300 time: 1.9687s\n",
      "Epoch: 0684 loss_train: 0.5712 acc_train: 0.8214 loss_val: 0.6563 acc_val: 0.8300 time: 1.9598s\n",
      "Epoch: 0685 loss_train: 0.6369 acc_train: 0.7929 loss_val: 0.6568 acc_val: 0.8267 time: 1.9348s\n",
      "Epoch: 0686 loss_train: 0.5717 acc_train: 0.8429 loss_val: 0.6574 acc_val: 0.8267 time: 1.9159s\n",
      "Epoch: 0687 loss_train: 0.5617 acc_train: 0.8429 loss_val: 0.6579 acc_val: 0.8233 time: 1.9139s\n",
      "Epoch: 0688 loss_train: 0.5833 acc_train: 0.8357 loss_val: 0.6587 acc_val: 0.8200 time: 1.8824s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0689 loss_train: 0.6540 acc_train: 0.7786 loss_val: 0.6594 acc_val: 0.8200 time: 1.9318s\n",
      "Epoch: 0690 loss_train: 0.6000 acc_train: 0.8143 loss_val: 0.6601 acc_val: 0.8200 time: 1.8840s\n",
      "Epoch: 0691 loss_train: 0.6367 acc_train: 0.8071 loss_val: 0.6607 acc_val: 0.8200 time: 1.9606s\n",
      "Epoch: 0692 loss_train: 0.5640 acc_train: 0.8500 loss_val: 0.6611 acc_val: 0.8200 time: 1.9495s\n",
      "Epoch: 0693 loss_train: 0.7173 acc_train: 0.7786 loss_val: 0.6615 acc_val: 0.8200 time: 2.0293s\n",
      "Epoch: 0694 loss_train: 0.5990 acc_train: 0.8286 loss_val: 0.6616 acc_val: 0.8233 time: 1.8880s\n",
      "Epoch: 0695 loss_train: 0.6208 acc_train: 0.8286 loss_val: 0.6612 acc_val: 0.8200 time: 1.9294s\n",
      "Epoch: 0696 loss_train: 0.5860 acc_train: 0.8000 loss_val: 0.6608 acc_val: 0.8200 time: 2.0513s\n",
      "Epoch: 0697 loss_train: 0.6019 acc_train: 0.8286 loss_val: 0.6601 acc_val: 0.8233 time: 2.0556s\n",
      "Epoch: 0698 loss_train: 0.5352 acc_train: 0.8571 loss_val: 0.6593 acc_val: 0.8233 time: 1.9689s\n",
      "Epoch: 0699 loss_train: 0.6437 acc_train: 0.8000 loss_val: 0.6584 acc_val: 0.8233 time: 1.9458s\n",
      "Epoch: 0700 loss_train: 0.5899 acc_train: 0.8214 loss_val: 0.6578 acc_val: 0.8233 time: 2.1324s\n",
      "Epoch: 0701 loss_train: 0.6020 acc_train: 0.8214 loss_val: 0.6576 acc_val: 0.8233 time: 1.9772s\n",
      "Epoch: 0702 loss_train: 0.5948 acc_train: 0.8286 loss_val: 0.6575 acc_val: 0.8233 time: 2.0266s\n",
      "Epoch: 0703 loss_train: 0.5847 acc_train: 0.8429 loss_val: 0.6574 acc_val: 0.8233 time: 1.9871s\n",
      "Epoch: 0704 loss_train: 0.5128 acc_train: 0.9000 loss_val: 0.6572 acc_val: 0.8233 time: 1.9657s\n",
      "Epoch: 0705 loss_train: 0.5740 acc_train: 0.8429 loss_val: 0.6569 acc_val: 0.8267 time: 2.0076s\n",
      "Epoch: 0706 loss_train: 0.5198 acc_train: 0.8786 loss_val: 0.6567 acc_val: 0.8267 time: 1.9824s\n",
      "Epoch: 0707 loss_train: 0.6865 acc_train: 0.7857 loss_val: 0.6564 acc_val: 0.8267 time: 2.0985s\n",
      "Epoch: 0708 loss_train: 0.5279 acc_train: 0.8286 loss_val: 0.6565 acc_val: 0.8267 time: 1.9643s\n",
      "Epoch: 0709 loss_train: 0.5629 acc_train: 0.8000 loss_val: 0.6567 acc_val: 0.8267 time: 2.0246s\n",
      "Epoch: 0710 loss_train: 0.5641 acc_train: 0.8571 loss_val: 0.6569 acc_val: 0.8267 time: 2.1034s\n",
      "Epoch: 0711 loss_train: 0.5828 acc_train: 0.8143 loss_val: 0.6571 acc_val: 0.8267 time: 2.0196s\n",
      "Epoch: 0712 loss_train: 0.6458 acc_train: 0.8214 loss_val: 0.6576 acc_val: 0.8267 time: 2.0377s\n",
      "Epoch: 0713 loss_train: 0.6265 acc_train: 0.7929 loss_val: 0.6582 acc_val: 0.8267 time: 2.0776s\n",
      "Epoch: 0714 loss_train: 0.6048 acc_train: 0.8000 loss_val: 0.6590 acc_val: 0.8233 time: 1.9847s\n",
      "Epoch: 0715 loss_train: 0.6515 acc_train: 0.8071 loss_val: 0.6597 acc_val: 0.8233 time: 1.9907s\n",
      "Epoch: 0716 loss_train: 0.7156 acc_train: 0.7500 loss_val: 0.6602 acc_val: 0.8233 time: 1.9912s\n",
      "Epoch: 0717 loss_train: 0.7320 acc_train: 0.7714 loss_val: 0.6605 acc_val: 0.8233 time: 2.0096s\n",
      "Epoch: 0718 loss_train: 0.6237 acc_train: 0.8357 loss_val: 0.6607 acc_val: 0.8233 time: 2.0425s\n",
      "Epoch: 0719 loss_train: 0.5765 acc_train: 0.8714 loss_val: 0.6608 acc_val: 0.8233 time: 2.0096s\n",
      "Epoch: 0720 loss_train: 0.5983 acc_train: 0.8500 loss_val: 0.6605 acc_val: 0.8200 time: 2.0056s\n",
      "Epoch: 0721 loss_train: 0.5645 acc_train: 0.8571 loss_val: 0.6601 acc_val: 0.8200 time: 2.0608s\n",
      "Epoch: 0722 loss_train: 0.5813 acc_train: 0.8429 loss_val: 0.6596 acc_val: 0.8233 time: 2.0098s\n",
      "Epoch: 0723 loss_train: 0.5411 acc_train: 0.8643 loss_val: 0.6591 acc_val: 0.8233 time: 2.0452s\n",
      "Epoch: 0724 loss_train: 0.6044 acc_train: 0.8357 loss_val: 0.6584 acc_val: 0.8233 time: 1.9263s\n",
      "Epoch: 0725 loss_train: 0.5538 acc_train: 0.8286 loss_val: 0.6577 acc_val: 0.8267 time: 2.0626s\n",
      "Epoch: 0726 loss_train: 0.6470 acc_train: 0.8214 loss_val: 0.6571 acc_val: 0.8267 time: 1.9728s\n",
      "Epoch: 0727 loss_train: 0.6011 acc_train: 0.8214 loss_val: 0.6563 acc_val: 0.8267 time: 2.0334s\n",
      "Epoch: 0728 loss_train: 0.5946 acc_train: 0.8214 loss_val: 0.6558 acc_val: 0.8267 time: 2.0396s\n",
      "Epoch: 0729 loss_train: 0.6595 acc_train: 0.8214 loss_val: 0.6554 acc_val: 0.8267 time: 1.9427s\n",
      "Epoch: 0730 loss_train: 0.5673 acc_train: 0.8429 loss_val: 0.6549 acc_val: 0.8267 time: 2.0563s\n",
      "Epoch: 0731 loss_train: 0.5979 acc_train: 0.8500 loss_val: 0.6546 acc_val: 0.8267 time: 1.9906s\n",
      "Epoch: 0732 loss_train: 0.6760 acc_train: 0.7786 loss_val: 0.6542 acc_val: 0.8267 time: 1.9566s\n",
      "Epoch: 0733 loss_train: 0.6333 acc_train: 0.8286 loss_val: 0.6541 acc_val: 0.8233 time: 1.9921s\n",
      "Epoch: 0734 loss_train: 0.6281 acc_train: 0.8071 loss_val: 0.6545 acc_val: 0.8233 time: 1.9258s\n",
      "Epoch: 0735 loss_train: 0.6954 acc_train: 0.8143 loss_val: 0.6547 acc_val: 0.8267 time: 1.9298s\n",
      "Epoch: 0736 loss_train: 0.5413 acc_train: 0.8500 loss_val: 0.6552 acc_val: 0.8267 time: 2.0156s\n",
      "Epoch: 0737 loss_train: 0.5656 acc_train: 0.8214 loss_val: 0.6555 acc_val: 0.8233 time: 1.9667s\n",
      "Epoch: 0738 loss_train: 0.4883 acc_train: 0.9000 loss_val: 0.6560 acc_val: 0.8233 time: 2.0882s\n",
      "Epoch: 0739 loss_train: 0.7061 acc_train: 0.7429 loss_val: 0.6569 acc_val: 0.8233 time: 2.0172s\n",
      "Epoch: 0740 loss_train: 0.5668 acc_train: 0.8571 loss_val: 0.6577 acc_val: 0.8200 time: 2.0565s\n",
      "Epoch: 0741 loss_train: 0.6783 acc_train: 0.7857 loss_val: 0.6583 acc_val: 0.8233 time: 1.9558s\n",
      "Epoch: 0742 loss_train: 0.5829 acc_train: 0.8214 loss_val: 0.6587 acc_val: 0.8233 time: 1.9049s\n",
      "Epoch: 0743 loss_train: 0.6567 acc_train: 0.8000 loss_val: 0.6597 acc_val: 0.8233 time: 1.9687s\n",
      "Epoch: 0744 loss_train: 0.6152 acc_train: 0.8071 loss_val: 0.6603 acc_val: 0.8233 time: 1.9967s\n",
      "Epoch: 0745 loss_train: 0.6140 acc_train: 0.8357 loss_val: 0.6606 acc_val: 0.8233 time: 1.9981s\n",
      "Epoch: 0746 loss_train: 0.4895 acc_train: 0.8786 loss_val: 0.6607 acc_val: 0.8167 time: 1.9737s\n",
      "Epoch: 0747 loss_train: 0.6739 acc_train: 0.8071 loss_val: 0.6607 acc_val: 0.8167 time: 2.0535s\n",
      "Epoch: 0748 loss_train: 0.6119 acc_train: 0.8214 loss_val: 0.6608 acc_val: 0.8167 time: 2.0159s\n",
      "Epoch: 0749 loss_train: 0.6567 acc_train: 0.8429 loss_val: 0.6607 acc_val: 0.8200 time: 2.0943s\n",
      "Epoch: 0750 loss_train: 0.5750 acc_train: 0.8071 loss_val: 0.6605 acc_val: 0.8167 time: 2.0747s\n",
      "Epoch: 0751 loss_train: 0.6734 acc_train: 0.8000 loss_val: 0.6602 acc_val: 0.8167 time: 2.0227s\n",
      "Epoch: 0752 loss_train: 0.4954 acc_train: 0.8643 loss_val: 0.6602 acc_val: 0.8167 time: 1.9483s\n",
      "Epoch: 0753 loss_train: 0.5736 acc_train: 0.8500 loss_val: 0.6601 acc_val: 0.8267 time: 1.9708s\n",
      "Epoch: 0754 loss_train: 0.6153 acc_train: 0.8000 loss_val: 0.6599 acc_val: 0.8267 time: 1.9999s\n",
      "Epoch: 0755 loss_train: 0.5224 acc_train: 0.8500 loss_val: 0.6598 acc_val: 0.8267 time: 1.9637s\n",
      "Epoch: 0756 loss_train: 0.5268 acc_train: 0.8286 loss_val: 0.6597 acc_val: 0.8300 time: 2.0590s\n",
      "Epoch: 0757 loss_train: 0.5361 acc_train: 0.8786 loss_val: 0.6595 acc_val: 0.8300 time: 1.9429s\n",
      "Epoch: 0758 loss_train: 0.6230 acc_train: 0.8000 loss_val: 0.6597 acc_val: 0.8300 time: 1.9838s\n",
      "Epoch: 0759 loss_train: 0.5172 acc_train: 0.8357 loss_val: 0.6597 acc_val: 0.8300 time: 1.9677s\n",
      "Epoch: 0760 loss_train: 0.5849 acc_train: 0.8429 loss_val: 0.6597 acc_val: 0.8333 time: 1.9444s\n",
      "Epoch: 0761 loss_train: 0.6247 acc_train: 0.7929 loss_val: 0.6596 acc_val: 0.8333 time: 1.9887s\n",
      "Epoch: 0762 loss_train: 0.5984 acc_train: 0.8214 loss_val: 0.6597 acc_val: 0.8333 time: 1.9618s\n",
      "Epoch: 0763 loss_train: 0.5595 acc_train: 0.8571 loss_val: 0.6597 acc_val: 0.8333 time: 1.9239s\n",
      "Epoch: 0764 loss_train: 0.5692 acc_train: 0.8143 loss_val: 0.6601 acc_val: 0.8333 time: 1.9338s\n",
      "Epoch: 0765 loss_train: 0.5639 acc_train: 0.8786 loss_val: 0.6607 acc_val: 0.8267 time: 1.9331s\n",
      "Epoch: 0766 loss_train: 0.5665 acc_train: 0.7929 loss_val: 0.6617 acc_val: 0.8267 time: 2.0238s\n",
      "Epoch: 0767 loss_train: 0.6297 acc_train: 0.8286 loss_val: 0.6627 acc_val: 0.8267 time: 1.9219s\n",
      "Epoch: 0768 loss_train: 0.6717 acc_train: 0.8071 loss_val: 0.6639 acc_val: 0.8233 time: 1.9339s\n",
      "Epoch: 0769 loss_train: 0.6628 acc_train: 0.8071 loss_val: 0.6650 acc_val: 0.8233 time: 1.9338s\n",
      "Epoch: 0770 loss_train: 0.6390 acc_train: 0.8071 loss_val: 0.6658 acc_val: 0.8233 time: 1.8700s\n",
      "Epoch: 0771 loss_train: 0.5591 acc_train: 0.8429 loss_val: 0.6666 acc_val: 0.8233 time: 2.0306s\n",
      "Epoch: 0772 loss_train: 0.6876 acc_train: 0.8000 loss_val: 0.6673 acc_val: 0.8233 time: 2.0106s\n",
      "Epoch: 0773 loss_train: 0.6213 acc_train: 0.8000 loss_val: 0.6677 acc_val: 0.8267 time: 1.9159s\n",
      "Epoch: 0774 loss_train: 0.6487 acc_train: 0.8071 loss_val: 0.6682 acc_val: 0.8300 time: 1.8889s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0775 loss_train: 0.4916 acc_train: 0.8571 loss_val: 0.6685 acc_val: 0.8300 time: 1.9917s\n",
      "Epoch: 0776 loss_train: 0.5853 acc_train: 0.8286 loss_val: 0.6687 acc_val: 0.8300 time: 1.9239s\n",
      "Epoch: 0777 loss_train: 0.5871 acc_train: 0.8214 loss_val: 0.6686 acc_val: 0.8233 time: 2.0246s\n",
      "Epoch: 0778 loss_train: 0.5753 acc_train: 0.8571 loss_val: 0.6686 acc_val: 0.8200 time: 1.8780s\n",
      "Epoch: 0779 loss_train: 0.6818 acc_train: 0.7929 loss_val: 0.6686 acc_val: 0.8200 time: 1.8991s\n",
      "Epoch: 0780 loss_train: 0.7224 acc_train: 0.8214 loss_val: 0.6685 acc_val: 0.8200 time: 1.9697s\n",
      "Epoch: 0781 loss_train: 0.5112 acc_train: 0.9000 loss_val: 0.6685 acc_val: 0.8200 time: 2.0018s\n",
      "Epoch: 0782 loss_train: 0.6965 acc_train: 0.7786 loss_val: 0.6682 acc_val: 0.8167 time: 1.8760s\n",
      "Epoch: 0783 loss_train: 0.6606 acc_train: 0.8143 loss_val: 0.6678 acc_val: 0.8167 time: 1.8999s\n",
      "Epoch: 0784 loss_train: 0.6443 acc_train: 0.7786 loss_val: 0.6670 acc_val: 0.8167 time: 1.9268s\n",
      "Epoch: 0785 loss_train: 0.5469 acc_train: 0.8571 loss_val: 0.6665 acc_val: 0.8167 time: 2.0655s\n",
      "Epoch: 0786 loss_train: 0.6923 acc_train: 0.7857 loss_val: 0.6660 acc_val: 0.8167 time: 1.9565s\n",
      "Epoch: 0787 loss_train: 0.5859 acc_train: 0.8500 loss_val: 0.6656 acc_val: 0.8200 time: 1.9288s\n",
      "Epoch: 0788 loss_train: 0.5034 acc_train: 0.8857 loss_val: 0.6654 acc_val: 0.8300 time: 1.9548s\n",
      "Epoch: 0789 loss_train: 0.6204 acc_train: 0.8357 loss_val: 0.6654 acc_val: 0.8300 time: 2.0002s\n",
      "Epoch: 0790 loss_train: 0.5760 acc_train: 0.8357 loss_val: 0.6655 acc_val: 0.8300 time: 2.1074s\n",
      "Epoch: 0791 loss_train: 0.5700 acc_train: 0.8500 loss_val: 0.6655 acc_val: 0.8300 time: 2.2061s\n",
      "Epoch: 0792 loss_train: 0.5467 acc_train: 0.8143 loss_val: 0.6653 acc_val: 0.8300 time: 2.2859s\n",
      "Epoch: 0793 loss_train: 0.6468 acc_train: 0.8143 loss_val: 0.6647 acc_val: 0.8300 time: 2.0395s\n",
      "Epoch: 0794 loss_train: 0.5333 acc_train: 0.8786 loss_val: 0.6641 acc_val: 0.8300 time: 1.9468s\n",
      "Epoch: 0795 loss_train: 0.6058 acc_train: 0.8143 loss_val: 0.6637 acc_val: 0.8267 time: 1.9190s\n",
      "Epoch: 0796 loss_train: 0.5717 acc_train: 0.8286 loss_val: 0.6634 acc_val: 0.8267 time: 1.9239s\n",
      "Epoch: 0797 loss_train: 0.6177 acc_train: 0.7929 loss_val: 0.6632 acc_val: 0.8267 time: 1.9099s\n",
      "Epoch: 0798 loss_train: 0.5731 acc_train: 0.8214 loss_val: 0.6631 acc_val: 0.8267 time: 1.9080s\n",
      "Epoch: 0799 loss_train: 0.5850 acc_train: 0.8286 loss_val: 0.6630 acc_val: 0.8267 time: 1.9236s\n",
      "Epoch: 0800 loss_train: 0.6728 acc_train: 0.8286 loss_val: 0.6628 acc_val: 0.8267 time: 1.9749s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1567.3155s\n",
      "Loading 732th epoch\n",
      "Test set results: loss= 0.6624 accuracy= 0.8470\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Epoch: 0001 loss_train: 1.6168 acc_train: 0.5071 loss_val: 1.6278 acc_val: 0.5767 time: 31.6586s\n",
    "- Epoch: 0002 loss_train: 1.5773 acc_train: 0.5714 loss_val: 1.6154 acc_val: 0.5767 time: 30.0393s\n",
    "\n",
    "Optimization Finished!\n",
    "- Total time elapsed: 62.3707s\n",
    "- Loading 1th epoch\n",
    "- Test set results: loss= 1.6820 accuracy= 0.4440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.959144375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1567.3155/800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6624 accuracy= 0.8470\n"
     ]
    }
   ],
   "source": [
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
